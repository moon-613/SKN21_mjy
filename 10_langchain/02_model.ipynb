{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dec491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상환경\n",
    "# uv venv .venv --python=3.12\n",
    "\n",
    "# 활성화\n",
    "# source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd979cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 43ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install ipykernel ipywidgets\n",
    "# !uv pip install langchain_openai\n",
    "# !uv pip install langchain langchain-classic langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - **신용카드가 등록된 경우 바로 충전할 수 있다.**\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "   - 금액 입력 후 `continue`\n",
    "   - 다음 페이지에서 세금 포함 결제 금액 나오면 `Confirm`  한다.\n",
    "\n",
    "2. 신용카드 등록\n",
    "   - Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API Key Load -> OS 환경 변수로 등록 (OPENAI_API_KEY)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c087d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57ff49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI() # 컴퓨터 설정 (환경변수)에 저장되어 있다면 자동으로 읽어옴. api key가 환경변수에 없으면 직접 입력. \n",
    "# response: 인공지능이 보낼 대답을 담아둘 바구니 이름. \n",
    "# chat.completions: OpenAI가 제공하는 기능 중 '채팅 형태의 완성' 기능을 쓰겠다는 경로.\n",
    "# create: 실제로 질문을 생성해서 보내라는 명령어. \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\", # 사용할 모델 종류의 이름\n",
    "    # messages: 대화 내역을 담는 리스트 []. 인공지능이 이전 대화 맥락을 알아야 하므로 리스트 형태로 전달. \n",
    "    # {}: 누가: role, 무슨 말: content를 했는지 짝을 지어 표현. \n",
    "    # role : user = 나, 사용자  , content: 인공지능에게 전달할 실제 질문.\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"OpenAI의 LLM 모델 종류는 뭐가 있어?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약하면, OpenAI의 LLM은 크게 다음 계열로 나뉩니다. API에서 자주 쓰는 모델들 위주로 정리해 두었습니다.\n",
      "\n",
      "- GPT-4 시리즈\n",
      "  - GPT-4: 멀티모달 가능(텍스트 입력 외에 이미지 등 입력도 처리하는 버전), 일반 대화 및 고도화된 작업에 사용.\n",
      "  - GPT-4-turbo: 더 빠르고 비용 효율적인 버전.\n",
      "  - GPT-4o: 멀티모달(이미지/오디오 포함) 지원 버전으로 발표된 변형도 있습니다.\n",
      "- GPT-3.5 시리즈\n",
      "  - GPT-3.5-turbo: 대화형 용도로 가장 많이 쓰이는 고성능 모델.\n",
      "  - text-davinci-003: 이전 세대의 강력한 텍스트 엔진(지금은 보완/대체되는 경우가 많음).\n",
      "- Codex(코드 전문) 계열\n",
      "  - code-davinci-002\n",
      "  - code-cushman-001\n",
      "  - GitHub Copilot 등 코드 보조에 주로 사용되는 모델들.\n",
      "- 과거/레거시 엔진(참고용)\n",
      "  - GPT-3 계열의 ada, babbage, curie, davinci 등: 다양한 규모의 텍스트 생성용 엔진으로 쓰임.\n",
      "  - text-curie-001, text-davinci-002 같은 텍스트 엔진도 과거에 널리 사용되었으나 현재는 주력 모델이 아닌 경우가 많습니다.\n",
      "- 기타 주의점\n",
      "  - DALL·E(이미지 생성), Whisper(음성 인식) 등은 LLM이지만 텍스트 기반 대화형 모델과 구분되는 멀티모달/다른 기능 모델들입니다.\n",
      "  - 모델의 가용성은 지역/계정에 따라 다르며, OpenAI의 최신 문서를 통해 현재 사용 가능한 엔진을 확인하는 것이 좋습니다.\n",
      "\n",
      "혹시 특정 사용 사례(예: 대화형 챗봇, 코드 보조, 요약/정리, 창의적 글쓰기 등)에 맞는 최적의 모델이 어떤지 원하시면 말씀해 주세요. 예에 맞춰 구체적인 추천도 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# response라는 바구니 안에 담긴 답변 내용만 꺼내 출력.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "        - gpt-5 모델은 추론과정을 거칠 수있고 이 경우 토큰을 소비한다. 그래서 max_tokens를 너무 작게 잡으면 응답이 안올 수 있다.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 돼있으면 생략. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공. 이것을 이용하면 특정 작업에 적합한 입력값을 설정 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "# 질의 \n",
    "response = model.invoke(\"Langchain은 어떤 Framework인지 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음처럼 요약해볼 수 있어요.\n",
      "\n",
      "- LangChain이란\n",
      "  - 대형 언어 모델(Large Language Model, LLM)을 이용한 애플리케이션을 쉽게 구축하도록 돕는 프레임워크/라이브러리예요.\n",
      "  - LLM 호출, 프롬프트 작성, 데이터 소스 연결, 외부 도구(툴) 사용, 기억(memory) 관리 등을 잘 엮어주는 “연동(glue)” 역할을 합니다.\n",
      "\n",
      "- 왜 필요한가\n",
      "  - 단순히 하나의 LLM 호출이 아니라, 여러 단계의 처리(프롬프트 템플릿 작성 → LLM 호출 → 결과 가공)나 도구 사용, 문맥 기억, 벡터 검색 등 다양한 기능을 조합해 복잡한 워크플로우를 쉽게 구축할 수 있게 해줍니다.\n",
      "  - Retrieval-Augmented Generation(RAG)처럼 외부 데이터에서 정보를 조회해 답을 만드는 패턴도 쉽게 구현 가능하게 해줍니다.\n",
      "\n",
      "- 핵심 구성 요소\n",
      "  - 프롬프트(Templates): 사용자의 입력이나 맥락에 맞춘 프롬프트를 미리 정의하고 재사용합니다.\n",
      "  - 체인(Chains): 여러 프롬프트+LLM 호출을 순차적으로 연결한 파이프라인. 간단한 LLMChain부터 복잡한 다단계까지 가능.\n",
      "  - 에이전트(Agents)와 도구(Tools): 에이전트가 상황에 따라 필요한 도구(API 호출, 데이터 조회 등)을 선택해 실행하는 의사결정 루프를 구현합니다.\n",
      "  - 기억/메모리(Memory): 대화 맥락이나 상태를 유지해 연속 대화나 맥락 기반 처리를 가능하게 합니다.\n",
      "  - 벡터 저장소와 임베딩(Embeddings/Vector Stores): 문서나 지식 베이스를 벡터화해 검색하고, RAG 방식으로 정보를 가져와 답을 생성하는 데 사용합니다.\n",
      "  - 데이터 로더 및 도큐먼트 인덱싱: 다양한 형식의 문서(웹 페이지, PDF 등)를 로딩하고 인덱싱하는 도구들.\n",
      "  - 언어/환경: Python용과 TypeScript/JavaScript용 두 가지 주요 구현이 있습니다. 각각의 생태계에서 필요한 모듈들이 제공됩니다.\n",
      "\n",
      "- 일반적인 사용 사례\n",
      "  - 문서 질의응답: 내부 지식베이스나 문서를 벡터 검색으로 찾아 LLM으로 답 생성.\n",
      "  - 대화형 에이전트: 사용자의 요청에 따라 API를 호출하거나 도구를 사용해 작업을 자동화.\n",
      "  - 코드 보조/리팩토링 도우미, 데이터 추출 및 변환 파이프라인 등 LLM 기반 자동화 도구\n",
      "  - 복잡한 워크플로우를 구성하는 프로토타이핑 및 생산용 애플리케이션\n",
      "\n",
      "- 간단한 예시 흐름\n",
      "  - RetrievalQA: 사용자가 질문을 하면 벡터 저장소를 검색해 관련 문서를 찾고, 그 문맥을 포함한 프롬프트로 LLM에 질의해 답을 생성.\n",
      "  - 간단한 체인 예: 프롬프트 템플릿 → LLM 호출 → 결과 가공 → 최종 출력\n",
      "  - 에이전트 예: 에이전트가 상황에 맞춰 API를 호출하고, 필요하면 다시 프롬프트를 구성해 LLM에게 재질문하는 루프\n",
      "\n",
      "- 시작 방법(간단 가이드)\n",
      "  - Python 버전: LangChain은 Python용 버전이 널리 쓰입니다. pip로 설치해서 시작하는 것이 일반적입니다.\n",
      "  - JavaScript/TypeScript 버전: 웹/노드 환경에서도 사용 가능하며, @langchain 계열 패키지로 시작하는 경우가 많습니다.\n",
      "  - 기본적인 예제나 공식 문서를 따라가면서 프롬프트 템플릿 작성, LLM 연결, 간단한 체인 생성부터 시작하는 것을 권장합니다.\n",
      "\n",
      "- 주의/팁\n",
      "  - LangChain은 많은 기능을 추상화해 편리하지만, 복잡한 워크플로우를 구성하다 보면 추상 수준이 높아져 학습 곡선이 다소 있습니다.\n",
      "  - 외부 LLM 제공자(API 키, 비용, 정책)에 따라 동작이 달라지므로, 적절한 비용 관리와 보안 설정이 필요합니다.\n",
      "\n",
      "간단 코드 예시(참고용, 버전에 따라 API 명칭이 조금 다를 수 있습니다)\n",
      "- Python\n",
      "\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "prompt = PromptTemplate(template=\"질문: {question}\\n답:\", input_variables=[\"question\"])\n",
      "llm = OpenAI(model=\"gpt-4\", temperature=0)\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "answer = chain.run({\"question\": \"LangChain이 무엇인가요?\"})\n",
      "print(answer)\n",
      "\n",
      "필요하시면 특정 사용 사례나 언어(Python vs TypeScript)에 맞춘 간단한 예제를 더 자세히 설명해 드릴게요. 또한 시작하기 좋은 공식 문서 링크나 설치 방법도 안내해 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285aea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\", streaming=True)\n",
    "\n",
    "res = model.stream(\"Langchain은 어떤 Framework인지 설명해줘.\")\n",
    "for token in res:\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ab46d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Your organization must be verified to generate reasoning summaries. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'reasoning.summary', 'code': 'unsupported_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# reasoning 설정 (gpt-5 추가)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m## reasoning: 문제 전제 -> 논리를 전개 -> 결론을 도출하는 과정을 스스로 계획/구성해서 답을 도출하는 방식\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# effort: 추론의 깊이 (low, medium, high), summary: 추론 과정 출력.\u001b[39;00m\n\u001b[32m      5\u001b[39m model = ChatOpenAI(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     reasoning={\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meffort\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     }\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLLM 모델에 대한 내용을 초심자를 위해 최대한 쉽게 설명해줘.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1382\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1381\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1385\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1387\u001b[39m ):\n\u001b[32m   1388\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1364\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m     raw_response = \u001b[38;5;28mself\u001b[39m.root_client.responses.with_raw_response.parse(\n\u001b[32m   1361\u001b[39m         **payload\n\u001b[32m   1362\u001b[39m     )\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m     raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m response = raw_response.parse()\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_response_headers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:866\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    830\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    865\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Your organization must be verified to generate reasoning summaries. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'reasoning.summary', 'code': 'unsupported_value'}}"
     ]
    }
   ],
   "source": [
    "# reasoning 설정 (gpt-5 추가)\n",
    "## reasoning: 문제 전제 -> 논리를 전개 -> 결론을 도출하는 과정을 스스로 계획/구성해서 답을 도출하는 방식\n",
    "# effort: 추론의 깊이 (low, medium, high), summary: 추론 과정 출력.\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\n",
    "        \"effort\": \"medium\", \"summary\":\"auto\"\n",
    "    }\n",
    ")\n",
    "result = model.invoke(\"LLM 모델에 대한 내용을 초심자를 위해 최대한 쉽게 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5193174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning\n",
    "# content에 추론 내용과 응답 내용을 묶어서 반환\n",
    "# type: reasoning - 추론 과정, text - 최종 응답. \n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들에 대해서 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "결과는 다음 답변 형식에 맞춘다. \n",
    "[답변 형식]\n",
    "- 행성 한국어 이름 (영어 이름): 간단한 한 줄 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7620488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성 (Mercury): 태양에서 가장 가까운 소형 암석 행성으로 얇은 대기와 많은 충돌분화구를 가짐\n",
      "- 금성 (Venus): 두꺼운 이산화탄소 대기와 강렬한 온실효과로 매우 뜨거운 암석 행성\n",
      "- 지구 (Earth): 액체 물과 생명이 존재하는 유일한 알려진 행성, 산소가 풍부한 대기 보유\n",
      "- 화성 (Mars): 붉은 색을 띠는 암석 행성으로 과거 물의 흔적과 거대한 협곡·화산이 있음\n",
      "- 목성 (Jupiter): 태양계에서 가장 큰 가스 거대행성으로 강력한 자기장과 거대한 소용돌이(대적반) 보유\n",
      "- 토성 (Saturn): 뚜렷한 고리계를 가진 가스 거대행성으로 밀도가 낮음\n",
      "- 천왕성 (Uranus): 얼음 성분이 많은 얼음 거대행성으로 자전축이 크게 기울어져 옆으로 도는 특징\n",
      "- 해왕성 (Neptune): 강한 바람과 푸른 색을 띠는 얼음 거대행성으로 태양에서 가장 멀리 있음\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface transformers -qU\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install torch==2.8\n",
    "# 수업에서는 2.8하라고 했는데 나는 안돼서 2.2 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 164ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d74c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 142ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 625ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/c1/rj7tb8cj0031l588c7fyvw0w0000gn/T/ipykernel_25202/550478833.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/bin/python: No module named uv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model_id)\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    task=\"text-generation\",\n",
    "    model_id=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bf770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 115ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.26.4\n",
      "Location: /Users/jiyouxg/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages\n",
      "Requires:\n",
      "Required-by: langchain-community, transformers\n"
     ]
    }
   ],
   "source": [
    "# !uv pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingface를 소개해줘\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:375\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    366\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m     **kwargs: Any,\n\u001b[32m    372\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    373\u001b[39m     config = ensure_config(config)\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    386\u001b[39m         .text\n\u001b[32m    387\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:786\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    779\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1008\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     run_managers = [\n\u001b[32m    991\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    992\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1006\u001b[39m         )\n\u001b[32m   1007\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1016\u001b[39m     run_managers = [\n\u001b[32m   1017\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1018\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1025\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1026\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:812\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    802\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    803\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    808\u001b[39m     **kwargs: Any,\n\u001b[32m    809\u001b[39m ) -> LLMResult:\n\u001b[32m    810\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    811\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m812\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    816\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    819\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    820\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    821\u001b[39m         )\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    823\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:332\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    329\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:127\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    126\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/10_langchain/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:483\u001b[39m, in \u001b[36mTextGenerationPipeline.postprocess\u001b[39m\u001b[34m(self, model_outputs, return_type, clean_up_tokenization_spaces, continue_final_message, skip_special_tokens)\u001b[39m\n\u001b[32m    481\u001b[39m input_ids = model_outputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    482\u001b[39m prompt_text = model_outputs[\u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m generated_sequence = \u001b[43mgenerated_sequence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m    484\u001b[39m records = []\n\u001b[32m    485\u001b[39m other_outputs = model_outputs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_outputs\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "res = model.invoke(\"Huggingface를 소개해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"서울에 주요 여행지 3곳을 알려줘.\"\n",
    "    }\n",
    "]\n",
    "res = model.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a067f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d70f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 343ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m     0 B/379.07 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[1A\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/44.65 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[2A\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[2A\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[2A\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[2A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 16.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 14.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 32.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 32.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 48.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 30.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 48.00 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 30.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 30.88 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 30.90 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 44.65 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 44.65 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2mlangchain-anthropic \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 44.65 KiB/44.65 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[3A\n",
      "\u001b[2mdocstring-parser    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 63.56 KiB/379.07 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 127.56 KiB/379.07 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)[2m-----------\u001b[0m\u001b[0m 239.56 KiB/379.07 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 198ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.75.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-anthropic\u001b[0m\u001b[2m==1.3.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic   # , Anthropic 지원하는 모델이 다른 것 같다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model=\"claude-sonnet-4-5\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m27 packages\u001b[0m \u001b[2min 308ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/14.02 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB         \u001b[1A\n",
      "\u001b[2mollama              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/28.52 KiB           \u001b[2A\n",
      "\u001b[2mollama              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 14.88 KiB/28.52 KiB         \u001b[2A\n",
      "\u001b[2mollama              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 28.52 KiB/28.52 KiB         \u001b[2A\n",
      "\u001b[2mollama              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 28.52 KiB/28.52 KiB         \u001b[2A\n",
      "\u001b[2mollama              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.02 KiB/14.02 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 28.52 KiB/28.52 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-ollama\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma3:1b\"   # ollama run 하고 실행할 때 사용한 이름.\n",
    ")\n",
    "res = model.invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.contect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI, 즉 인공지능은 단순히 인간처럼 생각하는 기계를 만드는 것을 의미하는 것이 아니라, **컴퓨터가 인간처럼 학습, 문제 해결, 의사 결정을 내릴 수 있도록 설계된 기술**입니다. 좀 더 자세히 풀어 설명하자면 다음과 같습니다.\\n\\n**1. AI의 기본 개념**\\n\\n* **인간처럼 생각하고 행동하도록 만든 시스템:** AI는 특정 문제에 대해 스스로 학습하고 행동하는 능력을 갖도록 구축된 프로그램입니다.\\n* **데이터를 기반으로 학습:** AI는 많은 데이터를 제공하여 학습합니다. 이 데이터를 통해 패턴을 파악하고, 지식을 축적합니다.\\n* **문제 해결 및 의사 결정:** 학습된 지식을 기반으로, 새로운 데이터를 분석하고 결론을 내려, 문제를 해결하거나 의사 결정을 내리는 기능을 수행합니다.\\n\\n**2. AI의 종류와 분야**\\n\\nAI는 크게 다음과 같은 유형으로 분류될 수 있으며, 각 분야에서 다양한 기술을 융합하여 활용되고 있습니다.\\n\\n*   **강건 AI (Strong AI) / 일반 AI:** 현재까지는 꿈에 해당하는 수준으로, 인간처럼 학습하고 상식을 활용하여 실현 능력이 있는 인공지능 모델입니다.\\n*   **약건 AI (Weak AI) / 좁은 AI:** 현재 가장 발전한 단계로, 특정 작업을 수행하기 위해 학습된 AI 모델입니다. 예를 들어, 이미지 인식, 음성 인식, 번역, 추천 등에 사용되는 모델들이 여기에 해당합니다.\\n*   **특정 분야 AI (Narrow AI) 또는 예시 AI:** 특정 과제에 특화되어 성능이 뛰어난 AI입니다. 예를 들어, 구글 번역, 스وام스, 구글 어시스턴트 등이 있습니다.\\n\\n**3. AI의 주요 기술 및 응용 분야**\\n\\n*   **머신러닝 (Machine Learning):** 데이터를 기반으로 학습하여 패턴을 찾고 예측하는 기술입니다.\\n    *   **지도 학습:** 학습 데이터를 통해 모델이 학습하도록 하는 방식\\n    *   **비지도 학습:** 데이터에서 패턴을 찾아 학습하도록 하는 방식\\n    *   **강화 학습:** 주어진 결과를 기반으로 시행착오를 통해 학습하도록 하는 방식 (게임, 롤플레잉 등)\\n*   **딥러닝 (Deep Learning):** 인공 신경망을 이용하여 복잡한 데이터를 학습하는 기술입니다. 특히 이미지, 언어, 음성의 딥러닝 모델이 개발되어 활용되고 있습니다.\\n*   **자연어 처리 (Natural Language Processing):** 인간의 언어를 컴퓨터에게 이해하고 해석하는 기술입니다. (챗봇, 기사 번역, 음성 인식 등)\\n*   **컴퓨터 비전 (计算机视觉):** 이미지 및 비디오 데이터를 이해하고 해석하는 기술입니다. (자율 주행, 객체 인식, 영상 분석 등)\\n*   **로보틱스 (Robotics):**  AI를 활용하여 로봇을 설계, 개발, 운용하는 기술입니다.\\n\\n**4. AI가 가져올 미래의 변화**\\n\\nAI는 우리 삶을 더욱 편리하게 만들고 효율적으로 만들 것입니다. 하지만 동시에 일자리에 영향을 미치는 등 다양한 문제점도 제기될 수 있으므로, AI 개발과 활용에 대한 지속적인 논의와 관점이 필요합니다.\\n\\n**더 궁금하신 점이 있다면 아래와 같이 질문해주세요.**\\n\\n*   특정 분야의 AI에 대해 알고 싶은 게 있을까요?\\n*   AI가 현재 우리 생활과 어떻게 영향을 주고 있나요?\\n*   AI의 윤리적인 문제를 해결하기 위한 노력은 무엇인가요?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-15T06:16:13.343388Z', 'done': True, 'done_reason': 'stop', 'total_duration': 40067836421, 'load_duration': 566995102, 'prompt_eval_count': 15, 'prompt_eval_duration': 410067615, 'eval_count': 761, 'eval_duration': 38154276441, 'logprobs': None, 'model_name': 'gemma3:1b', 'model_provider': 'ollama'}, id='lc_run--019b20a6-6086-7c72-aa54-dc7fb0c20706-0', usage_metadata={'input_tokens': 15, 'output_tokens': 761, 'total_tokens': 776})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.invoke(\"AI란 무엇인가요?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8951e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 안녕하세요! 😊 무엇을 도와드릴까요? 어떤 일이 있으신가요?\n",
      "\n",
      "(Hello! How's it going? ☺️ Do you want to chat or need anything?)\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "다음 내용을 3문장으로 요약 해줘.\n",
    "\n",
    "<요약할 내용>\n",
    "AI, 즉 인공지능은 단순히 인간처럼 생각하는 기계를 만드는 것을 의미하는 것이 아니라, **컴퓨터가 인간처럼 학습, 문제 해결, 의사 결정을 내릴 수 있도록 설계된 기술**입니다. 좀 더 자세히 풀어 설명하자면 다음과 같습니다.\\n\\n**1. AI의 기본 개념**\\n\\n* **인간처럼 생각하고 행동하도록 만든 시스템:** AI는 특정 문제에 대해 스스로 학습하고 행동하는 능력을 갖도록 구축된 프로그램입니다.\\n* **데이터를 기반으로 학습:** AI는 많은 데이터를 제공하여 학습합니다. 이 데이터를 통해 패턴을 파악하고, 지식을 축적합니다.\\n* **문제 해결 및 의사 결정:** 학습된 지식을 기반으로, 새로운 데이터를 분석하고 결론을 내려, 문제를 해결하거나 의사 결정을 내리는 기능을 수행합니다.\\n\\n**2. AI의 종류와 분야**\\n\\nAI는 크게 다음과 같은 유형으로 분류될 수 있으며, 각 분야에서 다양한 기술을 융합하여 활용되고 있습니다.\\n\\n*   **강건 AI (Strong AI) / 일반 AI:** 현재까지는 꿈에 해당하는 수준으로, 인간처럼 학습하고 상식을 활용하여 실현 능력이 있는 인공지능 모델입니다.\\n*   **약건 AI (Weak AI) / 좁은 AI:** 현재 가장 발전한 단계로, 특정 작업을 수행하기 위해 학습된 AI 모델입니다. 예를 들어, 이미지 인식, 음성 인식, 번역, 추천 등에 사용되는 모델들이 여기에 해당합니다.\\n*   **특정 분야 AI (Narrow AI) 또는 예시 AI:** 특정 과제에 특화되어 성능이 뛰어난 AI입니다. 예를 들어, 구글 번역, 스وام스, 구글 어시스턴트 등이 있습니다.\\n\\n**3. AI의 주요 기술 및 응용 분야**\\n\\n*   **머신러닝 (Machine Learning):** 데이터를 기반으로 학습하여 패턴을 찾고 예측하는 기술입니다.\\n    *   **지도 학습:** 학습 데이터를 통해 모델이 학습하도록 하는 방식\\n    *   **비지도 학습:** 데이터에서 패턴을 찾아 학습하도록 하는 방식\\n    *   **강화 학습:** 주어진 결과를 기반으로 시행착오를 통해 학습하도록 하는 방식 (게임, 롤플레잉 등)\\n*   **딥러닝 (Deep Learning):** 인공 신경망을 이용하여 복잡한 데이터를 학습하는 기술입니다. 특히 이미지, 언어, 음성의 딥러닝 모델이 개발되어 활용되고 있습니다.\\n*   **자연어 처리 (Natural Language Processing):** 인간의 언어를 컴퓨터에게 이해하고 해석하는 기술입니다. (챗봇, 기사 번역, 음성 인식 등)\\n*   **컴퓨터 비전 (计算机视觉):** 이미지 및 비디오 데이터를 이해하고 해석하는 기술입니다. (자율 주행, 객체 인식, 영상 분석 등)\\n*   **로보틱스 (Robotics):**  AI를 활용하여 로봇을 설계, 개발, 운용하는 기술입니다.\\n\\n**4. AI가 가져올 미래의 변화**\\n\\nAI는 우리 삶을 더욱 편리하게 만들고 효율적으로 만들 것입니다. 하지만 동시에 일자리에 영향을 미치는 등 다양한 문제점도 제기될 수 있으므로, AI 개발과 활용에 대한 지속적인 논의와 관점이 필요합니다.\\n\\n**더 궁금하신 점이 있다면 아래와 같이 질문해주세요.**\\n\\n*   특정 분야의 AI에 대해 알고 싶은 게 있을까요?\\n*   AI가 현재 우리 생활과 어떻게 영향을 주고 있나요?\\n*   AI의 윤리적인 문제를 해결하기 위한 노력은 무엇인가요?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f8f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/api-keys\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. `API Key 만들기` 선택\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `키이름 지정`하고 `가져온 프로젝트 선택`에서 프로젝트 선택(없으면 만든다) 하고 키 만들기\n",
    "4. 키가 생성되면 왼쪽에 복사 아이콘을 클릭하면 키가 복사된다.\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m36 packages\u001b[0m \u001b[2min 322ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/11.28 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB         \u001b[1A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[2A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[2A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[2A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[3A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[3A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[3A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/81.19 KiB           \u001b[3A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/177.01 KiB          \u001b[4A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/177.01 KiB          \u001b[4A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/177.01 KiB          \u001b[4A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/177.01 KiB          \u001b[4A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/177.01 KiB          \u001b[4A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2mpyasn1-modules      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/177.01 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/217.88 KiB          \u001b[5A\n",
      "\u001b[2mcachetools          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.28 KiB/11.28 KiB\n",
      "\u001b[2mrsa                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.88 KiB\n",
      "\u001b[2mpyasn1              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/81.19 KiB\n",
      "\u001b[2mpyasn1-modules      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/177.01 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)--------------\u001b[0m\u001b[0m     0 B/217.88 KiB          \u001b[5A\n",
      "\u001b[2mfiletype              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 14.83 KiB/19.50 KiB\n",
      "\u001b[2mrsa                   \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.89 KiB/33.88 KiB\n",
      "\u001b[2mlangchain-google-genai\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 59.76 KiB/62.06 KiB\n",
      "\u001b[2mpyasn1                \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 30.88 KiB/81.19 KiB\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.88 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.88 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 30.88 KiB/686.90 KiB      \u001b[8A\n",
      "\u001b[2mfiletype              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 14.83 KiB/19.50 KiB\n",
      "\u001b[2mrsa                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.88 KiB/33.88 KiB\n",
      "\u001b[2mlangchain-google-genai\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 59.76 KiB/62.06 KiB\n",
      "\u001b[2mpyasn1                \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 30.88 KiB/81.19 KiB\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.88 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 46.88 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 46.88 KiB/686.90 KiB      \u001b[8A\n",
      "\u001b[2mfiletype              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.50 KiB/19.50 KiB\n",
      "\u001b[2mrsa                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.88 KiB/33.88 KiB\n",
      "\u001b[2mpyasn1                \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 46.88 KiB/81.19 KiB\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.88 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 62.88 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 62.88 KiB/686.90 KiB      \u001b[7A\n",
      "\u001b[2mfiletype              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.50 KiB/19.50 KiB\n",
      "\u001b[2mpyasn1                \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 46.88 KiB/81.19 KiB\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 30.88 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 62.88 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 62.88 KiB/686.90 KiB      \u001b[6A\n",
      "\u001b[2mpyasn1                \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 46.88 KiB/81.19 KiB\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 48.00 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 42.32 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 76.48 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 94.88 KiB/686.90 KiB      \u001b[5A\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 92.80 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 74.32 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.48 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 254.88 KiB/686.90 KiB     \u001b[4A\n",
      "\u001b[2mwebsockets            \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 140.80 KiB/169.04 KiB\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 94.12 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 156.48 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 286.88 KiB/686.90 KiB     \u001b[4A\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 110.12 KiB/177.01 KiB\n",
      "\u001b[2mgoogle-auth           \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 188.48 KiB/217.88 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 302.88 KiB/686.90 KiB     \u001b[3A\n",
      "\u001b[2mpyasn1-modules        \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 126.12 KiB/177.01 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)----------------\u001b[0m\u001b[0m 318.88 KiB/686.90 KiB     \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/9)[2m-------------\u001b[0m\u001b[0m 366.88 KiB/686.90 KiB     \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/9)-\u001b[2m-----------\u001b[0m\u001b[0m 424.56 KiB/686.90 KiB     \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m9 packages\u001b[0m \u001b[2min 204ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.55.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-google-genai\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b72f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",   # 무료: 분당 60회, 하루 1000회까지 무료\n",
    ")\n",
    "res = model.invoke(\"gemini와 gemma 모델의 차이는 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma 모델은 모두 Google에서 개발했지만, **목적, 규모, 성능, 가용성 및 대상 사용자**에서 명확한 차이를 가지고 있습니다. 간단히 말해, **Gemini는 Google의 최고 성능 플래그십 멀티모달 모델**이고, **Gemma는 Gemini의 연구를 기반으로 만들어진 더 작고 효율적인 \"오픈 모델\" 패밀리**입니다.\n",
      "\n",
      "다음 표는 두 모델의 주요 차이점을 요약합니다.\n",
      "\n",
      "| 특징               | Gemini                                                                 | Gemma                                                                            |\n",
      "| :----------------- | :--------------------------------------------------------------------- | :------------------------------------------------------------------------------- |\n",
      "| **목적 및 역할**   | Google의 **플래그십 AI 모델**<br>최고 성능의 범용 AI 시스템              | **개발자 및 연구자**를 위한 효율적이고 접근성 높은 \"오픈 모델\"                  |\n",
      "| **규모 및 복잡성** | **매우 큼 (Ultra, Pro, Nano 등 다양한 크기)**<br>복잡하고 강력한 아키텍처 | **비교적 작음 (2B, 7B 등)**<br>효율성과 성능 간의 균형에 초점                   |\n",
      "| **성능 및 능력**   | **최첨단(State-of-the-art)**<br>**멀티모달** (텍스트, 이미지, 오디오, 비디오 등) <br>복잡한 추론, 코드 생성, 창의적 작업 등                                   | **해당 규모 대비 매우 우수**<br>**주로 텍스트 중심** (파인튜닝으로 확장 가능)<br>빠른 추론, 미세조정(fine-tuning)에 적합                                                 |\n",
      "| **가용성 및 접근성** | **독점적(Proprietary) 모델**<br>Google 제품(Bard, Pixel) 및 Google Cloud(Vertex AI), Google AI Studio를 통한 API 형태로 제공 | **\"오픈 모델\" (Open Models)**<br>모델 가중치가 공개되어 누구나 다운로드, 실행, 미세조정 가능 (특정 라이선스 하에)<br>로컬 실행, 온디바이스 AI 등에 적합 |\n",
      "| **주요 사용 사례** | 고급 챗봇, 복잡한 문제 해결, 멀티모달 콘텐츠 이해 및 생성, 연구 개발 | 온디바이스 AI, 맞춤형 AI 애플리케이션 개발, 연구, 교육, 비용 효율적인 솔루션 |\n",
      "| **개발 기반**      | Google DeepMind의 독자적 연구 및 기술력                                | Gemini 개발에 사용된 **동일한 연구 및 기술을 기반**으로 개발됨                  |\n",
      "| **대상 사용자**    | 일반 사용자 (Bard), 기업/엔터프라이즈 개발자, 고급 AI 연구자            | AI 개발자, 연구자, 학생, 스타트업, 리소스 제약이 있는 환경                     |\n",
      "\n",
      "**요약하자면:**\n",
      "\n",
      "*   **Gemini**는 Google의 가장 강력하고 다재다능한 AI 모델로, 최상위 성능을 목표로 하며 주로 Google의 제품과 클라우드 서비스를 통해 제공됩니다.\n",
      "*   **Gemma**는 Gemini의 선진 기술을 활용하여 더 작고 효율적으로 만들어진 모델 패밀리로, 개발자와 연구자들이 자유롭게 사용하고 미세조정할 수 있도록 \"오픈 모델\" 형태로 공개되었습니다. 이는 Google의 AI 전문 지식을 더 넓은 커뮤니티에 확산시키고 혁신을 촉진하려는 목적을 가집니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma 모델은 모두 Google에서 개발했지만, **목적, 규모, 성능, 가용성 및 대상 사용자**에서 명확한 차이를 가지고 있습니다. 간단히 말해, **Gemini는 Google의 최고 성능 플래그십 멀티모달 모델**이고, **Gemma는 Gemini의 연구를 기반으로 만들어진 더 작고 효율적인 \"오픈 모델\" 패밀리**입니다.\n",
       "\n",
       "다음 표는 두 모델의 주요 차이점을 요약합니다.\n",
       "\n",
       "| 특징               | Gemini                                                                 | Gemma                                                                            |\n",
       "| :----------------- | :--------------------------------------------------------------------- | :------------------------------------------------------------------------------- |\n",
       "| **목적 및 역할**   | Google의 **플래그십 AI 모델**<br>최고 성능의 범용 AI 시스템              | **개발자 및 연구자**를 위한 효율적이고 접근성 높은 \"오픈 모델\"                  |\n",
       "| **규모 및 복잡성** | **매우 큼 (Ultra, Pro, Nano 등 다양한 크기)**<br>복잡하고 강력한 아키텍처 | **비교적 작음 (2B, 7B 등)**<br>효율성과 성능 간의 균형에 초점                   |\n",
       "| **성능 및 능력**   | **최첨단(State-of-the-art)**<br>**멀티모달** (텍스트, 이미지, 오디오, 비디오 등) <br>복잡한 추론, 코드 생성, 창의적 작업 등                                   | **해당 규모 대비 매우 우수**<br>**주로 텍스트 중심** (파인튜닝으로 확장 가능)<br>빠른 추론, 미세조정(fine-tuning)에 적합                                                 |\n",
       "| **가용성 및 접근성** | **독점적(Proprietary) 모델**<br>Google 제품(Bard, Pixel) 및 Google Cloud(Vertex AI), Google AI Studio를 통한 API 형태로 제공 | **\"오픈 모델\" (Open Models)**<br>모델 가중치가 공개되어 누구나 다운로드, 실행, 미세조정 가능 (특정 라이선스 하에)<br>로컬 실행, 온디바이스 AI 등에 적합 |\n",
       "| **주요 사용 사례** | 고급 챗봇, 복잡한 문제 해결, 멀티모달 콘텐츠 이해 및 생성, 연구 개발 | 온디바이스 AI, 맞춤형 AI 애플리케이션 개발, 연구, 교육, 비용 효율적인 솔루션 |\n",
       "| **개발 기반**      | Google DeepMind의 독자적 연구 및 기술력                                | Gemini 개발에 사용된 **동일한 연구 및 기술을 기반**으로 개발됨                  |\n",
       "| **대상 사용자**    | 일반 사용자 (Bard), 기업/엔터프라이즈 개발자, 고급 AI 연구자            | AI 개발자, 연구자, 학생, 스타트업, 리소스 제약이 있는 환경                     |\n",
       "\n",
       "**요약하자면:**\n",
       "\n",
       "*   **Gemini**는 Google의 가장 강력하고 다재다능한 AI 모델로, 최상위 성능을 목표로 하며 주로 Google의 제품과 클라우드 서비스를 통해 제공됩니다.\n",
       "*   **Gemma**는 Gemini의 선진 기술을 활용하여 더 작고 효율적으로 만들어진 모델 패밀리로, 개발자와 연구자들이 자유롭게 사용하고 미세조정할 수 있도록 \"오픈 모델\" 형태로 공개되었습니다. 이는 Google의 AI 전문 지식을 더 넓은 커뮤니티에 확산시키고 혁신을 촉진하려는 목적을 가집니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
