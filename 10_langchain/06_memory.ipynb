{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/19(금) 10:40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e145329",
   "metadata": {},
   "source": [
    "# Langchain의 Memory 기능\n",
    "\n",
    "**Memory**: 사용자가 대규모 언어 모델(LLM, Large Language Model)과 주고받은 대화 내용을 저장하고 이를 이후의 대화에 활용하는 기능.\n",
    "\n",
    "## 왜 Memory가 필요한가?\n",
    "\n",
    "- 기본적으로 LLM은 상태 비저장(stateless) 모델이다. \n",
    "  - 의미: 한 번의 질문에 대해 답변을 제공하고, 그 이후에는 해당 질문과 답변 내용을 기억하지 못한다. 따라서 사용자가 이전 대화에 기반한 후속 질문을 하면, 모델은 맥락을 이해하지 못하고 정확한 답변을 하지 못한다.\n",
    "- 이 문제를 해결하기 위해, 지금까지의 대화 내용을 저장하고 이후 질문을 할 때 함께 제공하여 맥락을 이어갈 수 있도록 하는 기능이 바로 Memory이다.\n",
    "\n",
    "## Memory의 동작 방식\n",
    "\n",
    "- 사용자의 질문과 LLM의 응답을 저장한다.\n",
    "- 이후 사용자가 새로운 질문을 하면, **저장된 이전 대화 내용과 함께 모델에 전달**하여 자연스러운 연속 대화가 가능하도록 한다.\n",
    "- **주의**:\n",
    "  - 대화 이력이 너무 길어지면 현재 질문과 관련없는 내용이 많아지게 되고 이것이 noise가 되어 LLM이 부정확한 응답을 할 수도 있다. \n",
    "  - LLM마다 입력으로 받을 수 있는 [**토큰(Token)** 수에 제한](https://platform.openai.com/docs/models/compare)이 있다. 그래서 대화 내용을 무한정 저장하고 전달할 수 없다.\n",
    "  - Close source llm을 사용하는 경우 대화 이력이 길어지면 그많은 많은 토큰을 입력하게 되어 비용이 늘어나게 된다.\n",
    "  - 그래서 위와 같은 이유로 대화 이력을 모두 보내기 보다 최근 일부만 보내거나 요약하는 등의 관리가 필요하다.\n",
    "\n",
    "![memory.png](figures/memory.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90006054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503d6ffb",
   "metadata": {},
   "source": [
    "# 메시지 저장소: ChatMessageHistory\n",
    "메시지 기록을 관리하는 객체로 어디에 저장느냐에 따라 여러 클래스들이 구현되어 제공된다.\n",
    "\n",
    "## 종류\n",
    "- **BaseChatMessageHistory**\n",
    "    - 모든 메시지 기록 저장소 클래스의 **기본(최상위) 클래스**이다. 메시지를 저장하고 검색하는 기능을 정의하고 있으며, 이 클래스를 상속받아 다양한 저장소 방식이 구현된다.\n",
    "- **InMemoryChatMessageHistory**\n",
    "    - 메시지를 **메모리에 저장**하는 방식이다. 속도가 빠르지만, 프로그램을 종료하면 저장된 메시지는 사라진다.\n",
    "- 외부 저장소 연동 \n",
    "    - Langchain은 다양한 **3rd-party 저장소**와 연동할 수 있다. 예를 들어 SQLite, PostgreSQL, Redis, MongoDB 등을 사용해 메시지를 영구적으로 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70bf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "# role 별 message 객체 \n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "# (\"user/human\", \"message\") -> human message(content=\"message\")ㅓ\n",
    "# (\"ai/assistant\", \"message\") -> AIMessage(content=\"message\") : LLM 응답 결과\n",
    "# (\"system\", \"message\") -> SystemMessage(content=\"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccad613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = InMemoryChatMessageHistory()   # 메세지 저장소 객체 생성\n",
    "# 추가\n",
    "message_history.add_message(SystemMessage(\"당신은 여행 가이드 어시스턴트입니다.\"))\n",
    "message_history.add_message(HumanMessage(\"서울의 여행지 세 곳을 추천해주세요.\"))\n",
    "message_history.add_message(AIMessage(\"경복궁, 남산, 창경궁을 추천합니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0847b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 여행 가이드 어시스턴트입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='서울의 여행지 세 곳을 추천해주세요.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='경복궁, 남산, 창경궁을 추천합니다.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 Message들 조회\n",
    "message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387fe34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message History로 RDB 사용\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from sqlalchemy import create_engine   # DB와 연결을 위해 필요\n",
    "\n",
    "# SQLite에 저장\n",
    "engine = create_engine(\"sqlite:///message_history.sqlite\")\n",
    "# mysql - pymysql 설치\n",
    "# engine = create_engine(\"mysql://username:password@127.0.0.1:3306/DB이름\")\n",
    "sql_message_history2 = SQLChatMessageHistory(\n",
    "    session_id=\"chat-2\",   # 대화 ID. 대화 별로 따로 메세지를 저장.\n",
    "    connection=engine\n",
    ")\n",
    "\n",
    "sql_message_history2.add_message(SystemMessage(\"너는 유능한 비서야.\"))\n",
    "sql_message_history2.add_message(HumanMessage(\"안녕.\"))\n",
    "sql_message_history2.add_message(AIMessage(\"무엇을 도와 드릴까요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda7a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 유능한 비서야.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='안녕.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='무엇을 도와 드릴까요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_message_history2.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaaab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ccbd9f7",
   "metadata": {},
   "source": [
    "# RunnableWithMessageHistory\n",
    "\n",
    "- **`RunnableWithMessageHistory`**는 `ChatMessageHistory`와 `Runnable Chain`을 이용해 대화 이력을 관리하면서 대화할 수있도록 처리하는 `Runnable` 이다.\n",
    "  - 대화형 애플리케이션에서는 사용자와 AI 간의 여러 번의 주고받는 대화를 통해 작업을 수행한다. 이때, 이전 대화 내용을 기억하지 못하면 일관성 없는 응답이 발생할 수 있다. \n",
    "  - `RunnableWithMessageHistory`는 이러한 문제를 해결하고 대화 흐름을 자연스럽게 유지하도록 설계되었다.\n",
    "\n",
    "## 특징\n",
    "\n",
    "- 체인이 실행될 때마다 **대화 메시지를 자동으로 기록**하여 개발자가 별도로 상태를 관리하지 않아도 된다.\n",
    "- `session_id`(대화 ID)를 사용하여 대화를 구분한다.\n",
    "  - 동일한 `session_id`를 사용하면 이전 대화를 이어갈 수 있다.\n",
    "  - 새로운 `session_id`를 사용하면 새로운 대화로 인식된다.\n",
    "\n",
    "## 생성\n",
    "\n",
    "`RunnableWithMessageHistory`는 다음과 같은 요소들을 initializer에 전달해 생성한다.\n",
    "\n",
    "- **runnable**: 실제 작업을 수행하는 체인(`Runnable`) 객체이다.\n",
    "- **get_session_history**: 주어진 `session_id`에 해당하는 메시지 기록 저장소(`ChatMessageHistory`) 객체를 반환하는 함수이다.\n",
    "- **input_messages_key**: 사용자 입력 메시지를 저장할 입력 필드의 이름이다.\n",
    "- **history_messages_key**: 저장된 이전 대화 메시지를 불러올 필드의 이름이다.\n",
    "\n",
    "이를 통해 체인을 실행할 때마다 이전 메시지가 자동으로 전달되고, 새로운 메시지도 기록된다.\n",
    "\n",
    "[Langchain Memory Integration 문서](https://python.langchain.com/docs/integrations/memory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71446e95-2594-44d5-bb0b-397f33336d7c",
   "metadata": {},
   "source": [
    "## RunnableWithMessageHistory를 이용해 Chain 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62ae910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m61 packages\u001b[0m \u001b[2min 392ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/465.22 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB        \u001b[1A\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[2A\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[2A\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[2A\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[3A\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[3A\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[3A\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[4A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[4A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[4A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[4A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[4A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/100.42 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/1.02 MiB            \u001b[5A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.96 KiB\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/82.72 KiB\n",
      "\u001b[2mlangchain           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/100.42 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 14.89 KiB/1.02 MiB          \u001b[5A\n",
      "\u001b[2mlangchain-openai    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 78.88 KiB/82.72 KiB\n",
      "\u001b[2mlangchain           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 78.92 KiB/100.42 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 110.92 KiB/267.30 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 62.89 KiB/1.02 MiB          \u001b[5A\n",
      "\u001b[2mlangchain           \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 94.92 KiB/100.42 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 158.92 KiB/267.30 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 64.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 62.89 KiB/1.02 MiB          \u001b[4A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 158.92 KiB/267.30 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 64.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 78.89 KiB/1.02 MiB          \u001b[3A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 158.92 KiB/267.30 KiB\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 80.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 78.89 KiB/1.02 MiB          \u001b[3A\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 192.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 126.89 KiB/1.02 MiB         \u001b[2A\n",
      "\u001b[2mlangchain-core      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 432.00 KiB/465.22 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 318.89 KiB/1.02 MiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 334.78 KiB/1.02 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)--------------\u001b[0m\u001b[0m 456.56 KiB/1.02 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)2m------------\u001b[0m\u001b[0m 616.56 KiB/1.02 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)----\u001b[2m------\u001b[0m\u001b[0m 808.56 KiB/1.02 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m6 packages\u001b[0m \u001b[2min 304ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m6 packages\u001b[0m \u001b[2min 416ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.1.6\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.59\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.13.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -U langchain langchain-core langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c25c46e-ec11-41d1-85f5-13623dca5e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9d5cc5-5fd3-47ff-be20-274395989551",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", (\"당신은 AI 분야의 전문 Assistant입니다.\"\n",
    "                    \"답변은 20단어 이내로 해주세요.\"\n",
    "                    \"정확한 답변을 모를 경우 모른다고 답하세요.\")),\n",
    "        MessagesPlaceholder(variable_name=\"history\", optional=True),   # (\"placeholder\", \"{history}\")\n",
    "        (\"user\",\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dee10b-99fb-4a7c-8245-0126cedb2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_history(session_id:str) -> SQLChatMessageHistory:\n",
    "    \"\"\"\n",
    "    session_id의 대화내역을 관리하는 ChatMessageHistory를 반환하는 함수\n",
    "    Args:\n",
    "        session_id(str): 대화 id\n",
    "    Return:\n",
    "        SQLChatMessageHistory\n",
    "    \"\"\"\n",
    "    engine = create_engine(\"sqlite:///chat_message_history.sqlite\")\n",
    "    message_history = SQLChatMessageHistory(\n",
    "        session_id = session_id,\n",
    "        connection=engine\n",
    "    )\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507f8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableWithMessageHistory 생성 (Runnable)\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,   # Prompt -> model\n",
    "    input_messages_key=\"query\",   # Prompt Template에서 사용자 질문을 넣을 input variable\n",
    "    history_message_key = \"history\",   # 기존 대화 내역을 넣을 input_variable\n",
    "    get_session_history=get_message_history   # session_id의 대화를 관리하는 ChatMessageHistory를 반환하는 callable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 호출 (llm 요청).invoke(input_data, config:session_id)\n",
    "res = chain_with_history.invoke(\n",
    "    {\"query\":\"내 이름은 문지영입니다.\"},   # input_data\n",
    "    {\"configurable\":{\"session_id\":\"chat-10\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109075e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 문지영님, 만나서 반갑습니다. 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2358057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='문지영님입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 207, 'prompt_tokens': 372, 'total_tokens': 579, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CoLdedUHkS8vefjRodt86hxRVHhya', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b34b1-9aba-76c3-9fb3-c41cca7bf80e-0', usage_metadata={'input_tokens': 372, 'output_tokens': 207, 'total_tokens': 579, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\":\"내 이름이 뭐지?\"}, {\"configurable\":{\"session_id\":\"chat-10\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7604f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='모르겠습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 205, 'prompt_tokens': 67, 'total_tokens': 272, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CoLe5v9toVbaxPUzO0NPPK5LSPhKd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b34b2-0417-7653-ba47-d2de8bd9239e-0', usage_metadata={'input_tokens': 67, 'output_tokens': 205, 'total_tokens': 272, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\":\"내 이름이 뭐지?\"}, {\"configurable\":{\"session_id\":\"chat-11\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107fece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>User: python 공부는 어떻게 해야해\n",
      "<<<AI: 기초 문법 익히고, 작은 프로젝트로 실습하고, 알고리즘 풀기, 라이브러리 학습, 매일 코딩하세요.\n",
      ">>>User: \n",
      "<<<AI: 기초 문법 익히고, 작은 프로젝트로 실습하며 매일 코딩하고 코드 리뷰·리팩토링 하세요.\n",
      ">>>>>종료<<<<<<\n"
     ]
    }
   ],
   "source": [
    "# 1. 대화 id를 입력\n",
    "session_id = input(\"대화 ID: \")\n",
    "config = {\"configurable\":{\"session_id\":session_id}}\n",
    "\n",
    "# 2. 대화 진행 -!quit 입력 시 종료\n",
    "query = input(\"User: \")\n",
    "while True:\n",
    "    if query == \"!quit\":\n",
    "        print(\">>>>>종료<<<<<<\")\n",
    "        break\n",
    "    res = chain_with_history.invoke({\"query\":query}, config)\n",
    "    print(\">>>User:\", query)\n",
    "    print(\"<<<AI:\", res.content)\n",
    "    query = input(\"User:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307a032",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (408936126.py, line 27)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn (\"history\":messages_history, \"query\":input_data['query'])\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# 메세지 히스토리를 줄여서 보내기. 요약 처리, 최근 몇 개의 대화 내역만 보내기\n",
    "# 최근 내역만 잘라내는 함수 - trimm_messages()\n",
    "##############################################################\n",
    "\n",
    "# 메세지를 trimming 하는 runnable\n",
    "from langchain_core.messages import trim_messages\n",
    "def trimming_history(input_data:dict) -> dict:\n",
    "    \"\"\"\n",
    "    RunnableWithMessageHistory의 runnable chain의 첫 번째 요소로 들어갈 함수\n",
    "    input dictionary에 메세지의 히스토리와입 입력 query를 전달 받는다.\n",
    "    {history_messages_key: 히스토리, input_messaeges_key: 입력 쿼리, ...}\n",
    "    message history를 trimming해서 다음 chain 요소(Prompr)에 전달.\n",
    "    \"\"\"\n",
    "    # MAX_TOKEN은 대략 어디까지 줄일지의 기준. trimming을 할 때 메세지를 중간에서 자르지 않는다.\n",
    "    messages_history = trim_messages(\n",
    "        input_data['history'],   # 전체 메세지 히스토리\n",
    "        max_tokens=100,          # 히스토리를 줄일 기준 token 수. 지정한 토큰 수가 넘어가면 줄인다. \n",
    "        strategy=\"last\",         # 메세지를 줄일 때 last: 최근 메세지를, first: 오래된 메세지를 남긴다.\n",
    "        token_counter=model,     # 어떤 LLM을 기준으로 토큰을 계산할 지, ChatModel을 전달. \n",
    "        include_system=True,     # 줄일 때 system message 포함 여부.\n",
    "        start_on=\"human\",        # 줄일 때 시작 메세지가 HumanMessage가 되게 한다. (ai: AIMessage)\n",
    "    )\n",
    "\n",
    "    # input_data['history'][-9:]  # 그대로 사용\n",
    "    # input_data['history'][:-9]  # llm에 전송해서 요약\n",
    "    return (\"history\":messages_history, \"query\":input_data['query'])\n",
    "\n",
    "runnable = trimming_history | prompt | model\n",
    "\n",
    "chain_history = RunnableWithMessageHistory(\n",
    "    runnable=runnable,\n",
    "    history_messages_key=\"history\",\n",
    "    input_messages_key=\"query\",\n",
    "    get_session_history=get_message_history\n",
    ")\n",
    "config = {\"configurable\":{\"session_id\":\"id-100\"}}\n",
    "# chain_history.invoke({\"query\":\"질문\"}, config=config)\n",
    "# {\"history\": 저장소에서 질문/답변 히스토리를 조회, \"query\":질문} -> (runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"User: \")\n",
    "while True:\n",
    "    if query == \"!quit\":\n",
    "        print(\">>>>>종료<<<<<<\")\n",
    "        break\n",
    "    res = chain_history.invoke({\"query\":query}, config)\n",
    "    print(\">>>User:\", query)\n",
    "    print(\"<<<AI:\", res.content)\n",
    "    query = input(\"User:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
