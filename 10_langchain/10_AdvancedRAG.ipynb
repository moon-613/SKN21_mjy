{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742de262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25/12/29(월) 9:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d844427",
   "metadata": {},
   "source": [
    "# Advanced RAG란 무엇인가\n",
    "\n",
    "**RAG**는 LLM(대규모 언어 모델)이 답을 만들 때, **외부 지식(문서/DB/웹 등)을 검색(Retrieval)해서 근거(Context)로 붙인 뒤 생성(Generation)**하는 구조이다. 그런데 기본 RAG(Naive RAG)는 실무에서 다음 문제가 있다.\n",
    "\n",
    "- 검색이 \"비슷한 것\"은 찾는데 \"정답 근거\"를 제대로 못 찾는 경우\n",
    "- 근거가 있어도 LLM이 **hallucination(환각)**를 섞거나, 근거와 다른 답을 하는 경우\n",
    "- 검색한 문서가 너무 길고 복잡해서 **정답에 필요한 부분**(**chunk**)을 LLM이 제대로 찾지 못해 답변 품질이 안좋은 경우\n",
    "\n",
    "**Advanced RAG**는 이런 \"현업형 문제\"를 줄이기 위해 **데이터베이스 구축 단계, 검색 전단계, 검색 후 단계, 생성 단계에서 고도화**하는 설계 패턴이다.\n",
    "기본 RAG(Naive RAG)가 \"사람이 대충 찾아준 참고자료로 글 쓰는 것\"이라면, Advanced RAG는 \"사서(검색) + 편집자(정제) + 팩트체커(검증) + 작가(생성)가 협업하는 파이프라인\"에 가깝다.\n",
    "\n",
    "## 종류\n",
    "\n",
    "### 검색 품질을 올리는 고급 Retrieval\n",
    "\n",
    "- **하이브리드 검색**: 키워드(BM25) + 벡터(임베딩) 조합\n",
    "- **멀티-쿼리/쿼리 재작성(Query rewriting)**: 질문을 더 잘 검색되게 바꿔 검색 성공률을 올림\n",
    "- **메타데이터 필터링**: 메타데이터 필터링을 통해 범위를 좁혀 **정확도**를 올림. 의미기반 검색과 키워드 검색을 조합한 효과.\n",
    "- **리랭킹(Re-ranking)**: 후보 문서를 많이 가져온 뒤, \"정답에 가까운 순서\"로 다시 정렬\n",
    "\n",
    "### Chunking/인덱싱 전략 고도화\n",
    "- 벡터 데이터 베이스 구축시 어떤 구조로 문서를 분할하고 Indexing 할지의 전략\n",
    "  - **구조 기반 분할**: 헤더/섹션/표/코드블록\n",
    "  - **계층형 인덱스**: 문서 요약 → 섹션 → 세부 chunk, Parent-Child 구조\n",
    "  - **윈도우 확장**: 필요 시 앞 뒤 문맥을 함께 붙임\n",
    "  \n",
    "### 검색 결과의 \"정제/조립\" 단계 추가\n",
    "\n",
    "- 중복 제거, 노이즈 제거, 관련 부분만 발췌\n",
    "- 여러 chunk를 **질문 관점으로 재구성**\n",
    "  → LLM에 그대로 던지는 게 아니라 \"답변에 쓰기 좋은 근거 묶음\"으로 편집하여 전달한다.\n",
    "\n",
    "### 생성 단계의 신뢰성 강화(가드레일)\n",
    "\n",
    "- **근거 기반 답변 강제**: 검색한 문서에 답변의 근거가 없으면 \"답을 모른다.\"고 답변하도록 프롬프트를 구성한다.\n",
    "- **인용/근거 스니펫 포함**: 어느 문서에서 답이 나왔는지 근거를 보여주도록 하여 답변의 신뢰성을 높인다.\n",
    "- **자기검증(Verification)**: 답을 만든 뒤 \"근거와 모순 없는지\" 재확인 하도록 한다.\n",
    "- **불확실성 처리**: 애매하면 추가 질문(Clarification)을 하거나 또는 답변 범위 제한한다.\n",
    "\n",
    "\n",
    "## Advanced RAG가 필요한 일반적인 경우\n",
    "\n",
    "- 문서가 많고(수천~수백만), 구조가 복잡한 경우(매뉴얼/규정/기술문서)\n",
    "- 질문이 단순 검색이 아니라 \"비교/조건/절차/근거 요구\"가 많은 복잡한 질문인 경우.\n",
    "- 최신성/정확성이 중요한 경우(정책, 금융, 의료, 보안, 운영 장애 대응)\n",
    "- \"대충 그럴듯한 답\"이 아니라 **근거가 있는 답**이 필수인 경우\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# VectorStore, Retriever 준비\n",
    "###############################################################\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef11987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, SparseVectorParams, VectorParams\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################\n",
    "# 데이터 준비\n",
    "##############################################################\n",
    "\n",
    "def load_and_split_olympic_data(file_path=\"data/olympic_wiki.md\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        olympic_text = fr.read()\n",
    "\n",
    "    # Split\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ],\n",
    "        # strip_headers=False, # 문서에 header 포함 여부(default: True - 제거)\n",
    "    )\n",
    "\n",
    "    return splitter.split_text(olympic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1b469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Vector DB 연결\n",
    "# retriever 생성\n",
    "#################################################################\n",
    "\n",
    "# collection 삭제후 생성 (데이터 넣지는 않음)\n",
    "def get_vectorstore(collection_name: str = \"olympic_info_wiki\"):\n",
    "\n",
    "    #######################################\n",
    "    # Qdrant Collection 생성 (sparse + dense)\n",
    "    #######################################\n",
    "    dense_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "    #삭제후 생성\n",
    "    if client.collection_exists(collection_name):\n",
    "        result = client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\"dense\": VectorParams(size=3072, distance=Distance.COSINE)},\n",
    "        sparse_vectors_config={ \n",
    "            \"sparse\": SparseVectorParams()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ######################################\n",
    "    # VectorStore 생성 (Hybrid 모드)\n",
    "    ######################################\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "    \n",
    "        embedding=dense_embeddings,\n",
    "        \n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    \n",
    "        vector_name=\"dense\",\n",
    "        sparse_vector_name=\"sparse\",\n",
    "    )\n",
    "    \n",
    "    ######################################\n",
    "    # Document들 추가\n",
    "    ######################################\n",
    "    documents = load_and_split_olympic_data()\n",
    "    vector_store.add_documents(documents=documents)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def get_retriever(vector_store, k: int = 5):\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad675c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bd8ce17",
   "metadata": {},
   "source": [
    "# Rerank\n",
    "\n",
    "## 개념\n",
    "\n",
    "- RAG의 정확도는 관련 정보의 컨텍스트 내 존재 유무가 아니라 순서가 중요하다. 즉, 관련 정보가 컨텍스트 내 상위권에 위치하고 있을 때 좋은 답변을 얻을 수 있다는 뜻이다. \n",
    "- **Rerank**는 RAG 시스템에서 **초기 검색 단계에서 추출된 후보 문서들의 순위를 재조정**하는 기법이다. \n",
    "- 벡터 유사도 기반의 빠른 1차 검색 후, 보다 정밀한 모델(예: Cross-encoder, LLM 등)을 활용해 질문과 검색된 문서간의 의미론적 관련성을 평가하여, 실제로 답변 생성에 가장 **적합한 문서들이 상위**에 오도록 순서를 다시 매긴다. 이를 통해 LLM이 더 정확하고 관련성 높은 정보를 바탕으로 답변을 생성할 수 있게 도와준다.\n",
    "\n",
    "## 방법\n",
    "\n",
    "- **Cross-encoder 기반 Rerank**  \n",
    "  - Cross Encoder를 이용해서 순위를 재 지정한다.\n",
    "  - Cross-encoder\n",
    "    - 질문과 문서를 같이 입력으로 받아 둘간의 유사도 점수를 예측하도록 학습한 모델.\n",
    "    - 학습을 두 문장의 유사도록 예측하도록 학습하였기 때문에 단순 유사도 검사 보다 두 문장간의 의미적 관련성등을 이용해 유사도를 예측하기 때문에 더 정확한 결과를 보인다.\n",
    "  - 1차적으로 검색한 문서와 질문간의 유사도를 **cross-encoder**로 다시 계산해서 문서의 순위를 재 조정한다.\n",
    "  \n",
    "  > - **Bi-encoder**\n",
    "  >     - 질문 (query)와 문서(document)를 각각 독립적으로 인코딩한 후, 벡터 간 유사도 계산\n",
    "  >     - Encoder 모델은 개별 문장을 입력받아 embedding vector를 출력한다. 질문과 문서를 각각 encoding한 뒤에 둘 간의 유사도를 계산한다.\n",
    "  \n",
    "  ![bi_crosss_encoder](figures/bi_cross_encoder.png)\n",
    "\n",
    "  \\[출처:https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/\\]\n",
    "\n",
    "- **LLM 기반 Rerank**  \n",
    "  GPT-3, GPT-4 등 LLM을 활용해 각 문서가 질문에 얼마나 부합하는지 평가하여 순위를 매긴다. 성능이 뛰어나지만 비용이 높고 응답 속도가 느릴 수 있다.\n",
    "## Rerank RAG 프로세스  \n",
    "1. 1차 검색(예: 임베딩 기반 벡터 검색)으로 상위 k개 문서 추출.  \n",
    "2. Reranker 모델에 질문-문서 쌍을 입력.  \n",
    "3. 각 쌍의 관련성 점수 산출 및 재정렬.  \n",
    "4. 상위 n개 문서를 LLM의 컨텍스트로 전달하여 답변 생성.\n",
    "\n",
    "## 장단점\n",
    "\n",
    "- **장점**  \n",
    "  - Rerank를 적용하면 단순 벡터 유사도 기반 검색보다 훨씬 정교하게 질문과 관련된 정보를 추출할 수 있다. \n",
    "  - 실제로 생성되는 답변의 품질이 크게 향상되며, 도메인 특화 정보나 복잡한 질의에도 높은 정확도를 보인다.\n",
    "\n",
    "- **단점**  \n",
    "  - Cross-encoder나 LLM 기반 Rerank는 연산량이 많아 실시간 응답이 필요한 대규모 서비스에선 속도 저하가 발생할 수 있다.\n",
    "  - 초기 검색 결과(상위 k개)에만 적용하므로, 1차 검색의 품질이 낮으면 Rerank 효과가 제한적일 수 있다.\n",
    "\n",
    "## CrossEncoder Reranker 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06ec44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a06683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b361e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e044a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83aac8f1",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embedding)\n",
    "## 개념\n",
    "- 질문(query)에 대한 가상의 답변 문서를 생성하고, 이 생성된 가상의 답변 문서를 임베딩하여 검색에 활용하는 기법이다.\n",
    "- 일반적인 RAG에서 검색은 질문을 임베딩하여 문서 임베딩과 직접 비교한다. \n",
    "- 질문(\"파리는 어떤 도시인가?\")과 답변 문서(\"파리는 프랑스의 수도이며...\")는 표현 방식이 다르다는 문제가 있다. \n",
    "- 즉 의미적으로는 관련있지만 벡터 공간(임베딩 벡터간의 유사서)에서는 거리가 멀 수가 있다.\n",
    "- 그래서 HyDE는 질문과 문서가 아니라 질문으로 가상의 답변 문서를 만들고 **가상의 답변문서와 저장된 문서들간의 유사도**를 비교한다.\n",
    "\n",
    "## HyDE 프로세스\n",
    "\n",
    "1. 가상 문서 생성\n",
    "   -  LLM을 사용해 질문에 대한 가상의 답변 문서 생성\n",
    "   -  이때 성능이 좋은 LLM을 사용하는 것이 좋다.\n",
    "2. 임베딩 변환\n",
    "   - `1`에서 생성한 가상 문서를 벡터로 임베딩\n",
    "3. 유사도 검색\n",
    "   - 가상 문서 임베딩으로 Vector Store에 저장된 문서들 중 유사한 문서를 검색\n",
    "4. 답변 생성\n",
    "   - 검색된 실제 문서를 바탕으로 최종 답변 생성\n",
    "\n",
    "## 장단점\n",
    "- **장점**\n",
    "  - 질문-문서 간 의미적 차이를 해결 해서 정확한 문서 검색 가능\n",
    "  - 질문 표현 방식에 덜 민감\n",
    "- **단점**\n",
    "  - 가상 문서의 품질에 따른 성능 편차가 발생한다.\n",
    "    - 가상 문서 생성 시 환각(hallucination) 위험\n",
    "  - 추가적인 LLM 호출로 인한 비용 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62840b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2dae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b672ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02cd59f8",
   "metadata": {},
   "source": [
    "#  MultiQueryRetriever\n",
    "\n",
    "## 개념\n",
    "- 하나의 사용자 질문으로 **여러 개의 다양한 질문을 생성하여 검색**을 수행하는 방법이다.\n",
    "- 단일 질문의 한계를 극복하고 다각도에서 관련 정보 검색할 수있다.\n",
    "  - 기본 RAG는 사용자의 질문의 질(quality)에 따라 검색 결과가 좌우된다.\n",
    "  - 사용자가 한 질문에만 의존하는 것이 아니라 그 질문을 바탕으로 **다양한 의미의 질문들을 생성해서 단일 질문이 가지는 표현의 한계를 보완**한다.\n",
    "    - 동일한 질문을 다른 각도에서 접근할 수있다.\n",
    "    - 다양한 어휘와 표현으로 질문을 재구성한다.\n",
    "- 예)\n",
    "  - **원본 질문**: \"딥러닝의 장점은 무엇인가?\"\n",
    "  - **생성된 질문들**:\n",
    "    1. \"딥러닝이 전통적인 머신러닝보다 나은 점은?\"\n",
    "    2. \"딥러닝을 사용하면 얻을 수 있는 이익은?\"\n",
    "    3. \"딥러닝의 주요 강점과 특징은?\"\n",
    "    4. \"딥러닝 기술의 핵심 우위는?\"\n",
    "## 실행 프로세스\n",
    "\n",
    "1. 질문 생성   \n",
    "   - LLM을 사용해 원본 질문을 3-5개의 서로 다른 질문으로 변환\n",
    "2. 병렬 검색\n",
    "   - 생성된 각 질문으로 독립적으로 문서 검색 수행\n",
    "3. 결과 통합\n",
    "   - 여러 검색 결과를 하나로 병합\n",
    "4. 중복 제거\n",
    "   - 동일한 문서가 여러 번 검색된 경우 중복 제거\n",
    "5. 최종 답변\n",
    "   - 통합된 문서 세트를 바탕으로 답변 생성\n",
    "\n",
    "## 장단점\n",
    "- **장점**\n",
    "    - 단일 질문으로 놓칠 수 있는 관련 문서 발견 수 있다.\n",
    "    - 사용자 질문 표현 방식의 한계 극복\n",
    "    - 더 포괄적이고 완전한 정보 검색 및 수집을 할 수있다.\n",
    "- **단점**\n",
    "    - 여러 번의 LLM 호출과 검색 수행이 실행 되므로 **계산비용, 토큰비용, 응답시간이 증가한다.**\n",
    "    - 생성된 질문의 품질에 따른 성능 편차가 있을 수 있다.\n",
    "    - 생성된 질문에 따라 원래 질문과 관련성 낮은 문서도 검색될 수 있어 최종 답변을 방해하는 노이즈가 증가할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df5b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2015bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185452e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e581ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47826ade",
   "metadata": {},
   "source": [
    "# MapReduce RAG 방식\n",
    "\n",
    "## 개요\n",
    "\n",
    "- RAG(Retrieval-Augmented Generation)에서 검색된 문서들 중 질문과 관련성이 높은 문서만을 선별하여 더 정확한 답변을 생성하는 방법이다.\n",
    "- 검색된 문서들 중에서 질문 답변에 실제로 도움이 되는 문서만을 LLM을 통해 선별한 후 전달하는 방식이다.\n",
    "\n",
    "## MapReduce 방식 프로세스\n",
    "1. Map (문서 검색)\n",
    "   - 벡터스토어에서 질문과 유사한 문서들을 의미적 유사도 검색으로 찾는다.\n",
    "   - 이 단계에서는 단순 벡터 유사도만 고려하므로 질문과 직접적인 관련이 없는 문서도 포함될 수 있다.\n",
    "2. Reduce (문서 선별 및 요약)\n",
    "   - 검색된 각 문서가 질문 답변에 실제로 도움이 되는지 LLM에게 평가 요청한다.\n",
    "   - 관련성이 높은 문서들만 선별하여 요약하거나 결합한다.\n",
    "   - 필요시 여러 문서의 정보를 통합하여 더 응답에 적합한 컨텍스트를 생성한다.\n",
    "3.  Generate (최종 답변 생성)\n",
    "    - 질문과 선별된 컨텍스트를 함께 LLM에 전달하여 최종 답변을 생성한다.\n",
    "\n",
    "## 장단점\n",
    "\n",
    "- **장점**\n",
    "  - **높은 정확도**: 질문과 직접 관련된 정보만 사용하여 더 정확한 답변을 생성한다.\n",
    "  - **노이즈 제거**: 유사하지만 관련 없는 정보로 인한 혼동을 방지한다.\n",
    "  - **컨텍스트 최적화**: 제한된 토큰 범위 내에서 가장 유용한 정보만 전달한다.\n",
    "  - **확장성**: 많은 문서가 검색되어도 중요한 정보만 선별하여 처리할 수 있다.\n",
    "- 단점\n",
    "  - **추가 비용**: 문서 선별을 위한 LLM 호출로 인한 비용이 증가한다.\n",
    "  - **처리 시간**: 문서 평가 단계가 추가되어 응답 속도가 저하된다.\n",
    "  - **복잡성**: 구현과 관리가 더 복잡하다.\n",
    "  - **의존성**: 문서 선별 성능이 LLM의 판단 능력에 크게 의존한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a07c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c49de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db372ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e3d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8a900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78c96dd",
   "metadata": {},
   "source": [
    "# Self Query Retriever\n",
    "\n",
    "- **Self-Query Retriever**는 사용자의 자연어 질문을 LLM을 통해 '의미 기반 검색 쿼리(semantic query)'와 '메타데이터 기반 필터 조건(metadata filter)'으로 자동 분해/구조화하여, 벡터 검색과 조건 기반 필터링을 동시에 수행하도록 설계된 Advanced RAG의 고급 Retriever 기법이다. 이를 통해 메타데이터의 속성을 정확히 반영한 정밀 검색이 가능하다.\n",
    "- 즉 사용자 질의로 부터 메타데이터의 필터 조회시 사용할 값을 추출하여 metadata 조건 기반 필터링을 할 수 있도록 한다.\n",
    "\n",
    "## Self Query Retriever의 구성 요소\n",
    "\n",
    "- **LLM (Large Language Model)**\n",
    "    - 자연어로 표현된 사용자 질문을 받아, 이를 메타데이터 조건과 필터링 조건이 포함된 정형 쿼리(Structured Query)로 변환하는 역할을 한다.\n",
    "- **StructuredQuery**\n",
    "    - 사용자의 질문을 기반으로 생성되는 구조화된 쿼리로 다음 두가지가 생성된다.\n",
    "      - 벡터 유사도 검색을 위한 **의미 기반 검색 쿼리**(semantic query)\n",
    "      - 문서 메타데이터를 이용한 **필터링 조건**(metadata filter)\n",
    "  \n",
    "- **Query Translator**\n",
    "    - 생성된 StructuredQuery를 특정 벡터 데이터베이스(Qdrant, Chroma 등)의 쿼리 언어로 번역하여, 실제 검색이 가능하도록 한다.\n",
    "- **Vector Database**\n",
    "    - 변환된 쿼리를 바탕으로 벡터 유사도 검색과 메타데이터 조건 필터링을 함께 수행하여 관련 문서를 반환한다.\n",
    "\n",
    "## Self Query Retriever 작동 원리\n",
    "\n",
    "![selfquery retriever](figures/selfquery_retriever.jpg)\n",
    "\n",
    "1. 사용자가 자연어 질의(Query)를 입력한다.\n",
    "2. LLM이 입력된 자연어 질문을 해석하여 **Query Constructor**가 **StructuredQuery**로 변환한다.\n",
    "3. **Query Translator**가 StructuredQuery를 **벡터 데이터베이스에서 이해할 수 있는 쿼리**로 변환한다.\n",
    "4. 벡터 데이터베이스가 변환된 쿼리에 따라 문서를 검색한다.\n",
    "5. 최종적으로, 검색 결과가 사용자에게 제공된다.\n",
    "\n",
    "## 사용 예시\n",
    "1. Query\n",
    "    - \"2023년에 발표된 OpenAI의 GPT 모델 관련 논문을 찾아줘.\"\n",
    "2. Query Constructor가 위 질문을 다음과 같은 형태의 StructuredQuery로 변환한다.\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "        \"query\": \"GPT 모델 논문\",\n",
    "        \"filter\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"key\": \"year\",\n",
    "                    \"match\": {\n",
    "                        \"value\": 2023\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"author\",\n",
    "                    \"match\": {\n",
    "                        \"value\": \"OpenAI\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "     - query: 벡터검색에 사용될 자연어 의미기반 쿼리(semantic query)\n",
    "     - filter: 메타데이터 기반 필터 조건\n",
    "3. 위의 StruncturedQuery는 Retriever와 연결된 Vector database의 검색 형식에 맞춰 query translator에 의해 변환 되고 이것을 이용해 검색을 진행한다. (형식은 DB 마다 다르다.)\n",
    "\n",
    "\n",
    "## Self Query Retriever의 장점\n",
    "\n",
    "- **정밀성**: 메타데이터 조건을 정확하게 지정하여 원하는 문서를 정밀하게 검색할 수 있다.\n",
    "- **효율성**: 메타데이터 필터링을 통해 관련 없는 문서를 미리 제거하여 검색 대상 문서의 후보 집합을 사전에 축소함으로써, 벡터 유사도 계산 대상이 줄어들고 전체 검색 연산량을 감소시킬 수 있다.\n",
    "- **사용자 편의성**: 사용자가 복잡한 쿼리 조건을 직접 작성하지 않고, 자연어로 간편하게 질문할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03dba7f",
   "metadata": {},
   "source": [
    "## 예제\n",
    "- 필요 패키지 설치\n",
    "  - pip install langchain langchain-community langchain-openai langchain-qdrant lark\n",
    "\n",
    "> ### Lark 패키지\n",
    "> \n",
    "> - Lark는 텍스트를 구조적으로 분석하기 위한 파싱 라이브러리로, 미리 정의한 문법에 따라 문장을 해석하여 정해진 형태(parse tree 또는 abstract syntax tree) 형태의 구조화된 결과를 생성한다.\n",
    "> - 이 라이브러리는 컴파일러, 인터프리터, DSL(도메인 특화 언어), 쿼리 언어, 수식 해석기처럼 구조화된 입력을 처리해야 하는 다양한 프로그램에서 활용된다.\n",
    "> - LangChain의 **SelfQueryRetriever**에서는 LLM이 생성한 쿼리 조건 문자열(output)을 석하여 메타데이터 필터 조건으로 변환하기 위해 Lark가 사용된다. Lark는 텍스트 형식의 조건을 분석하여 벡터 검색 시 사용되는 StructuredQuery의 filter(metadata filter) 구조로 변환해주는 역할을 담당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da64881",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store 생성\n",
    "\n",
    "COLLECTION_NAME = \"example\"\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"example\",\n",
    "    embedding=embedding_model\n",
    ")\n",
    "vectorstore.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaa4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
