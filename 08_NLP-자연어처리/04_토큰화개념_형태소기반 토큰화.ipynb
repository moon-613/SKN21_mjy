{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30f766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11/28(금) 10:05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee251a30",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "\n",
    "토큰화(tokenization)는 자연어를 **모델이 이해할 수 있는 또는 모델이 다룰 수있는 기본 단위(Token)** 분할하는 과정.   \n",
    "토큰으로 나누는 단위는 설계에 따라 문장, 어절, 형태소, 서브워드, 문자, 자모/알파벳 등 다양한 방식으로 나눌 수 있다.   \n",
    "- 예\n",
    "```bash\n",
    "원문: \"자연어 처리는 재미있다\"\n",
    "토큰화: [\"자연어\", \"처리\", \"는\", \"재미있다\"]\n",
    "```\n",
    "\n",
    "## 토큰화 방식\n",
    "- **단어 기반 토큰화(Word-Level Tokenization)**\n",
    "    - 어절(공백으로 구분) 또는 형태소 단위로 단어를 나누는 전통적인 방식이다.\n",
    "    - **한국어**는 교착어로 하나의 단어에 다양한 조사/어미가 결합된다. 그래서 어절단위로 토큰화할 경우 어휘사전의 크기가 기하급수적으로 늘어나는 문제가 있다.\n",
    "      - 예) \"학교\", \"학교가\", \"학교를\", \"학교에\", \"학교에서\", \"학교로\", \"학교의\", ...\n",
    "    - 이로 인해 미등록어휘(OOV - Out of Vocabulary)의 증가, 같은 의미를 가지는 단어들이 Vocab에 중복 등록, 메모리 낭비, 학습효율성 저하 등 다양한 문제가 생긴다.\n",
    "    - 그래서 **한국어의 경우 형태소 단위 토큰화**가 필요하다.\n",
    "\n",
    "- **서브워드 기반(Subword-level) — BPE, WordPiece, Unigram**\n",
    "    - Transformer 기반 모델(BERT, GPT, LLaMA 등)에서 표준으로 사용하는 방식.\n",
    "    - 단어를 기준으로 토큰화하지 않고 **문자(character)와 단어(word)의 중간 수준인 서브워드(subword) 단위로 토큰화**한다.\n",
    "    - **동작 원리**:\n",
    "        - 자주 등장하는 문자열 조합(서브워드)을 하나의 토큰으로 구성한다.\n",
    "        - 빈도가 높은 단어는 하나의 토큰으로, 빈도가 낮거나 희귀한 단어는 여러 서브워드로 분할한다.\n",
    "    - **예시**:\n",
    "        ```bash\n",
    "        입력: \"나는 밥을 먹었습니다. 나는 어제 밥을 했습니다.\"\n",
    "        \n",
    "        서브워드 토큰화 결과 (예시):\n",
    "        [\"나는\", \"밥\", \"을\", \"먹\", \"었\", \"습니다\", \".\", \"나는\", \"어제\", \"밥\", \"을\", \"하\", \"었\", \"습니다\", \".\"]\n",
    "        ```\n",
    "    - **장점**:\n",
    "        - **미등록 단어(OOV) 문제 해결**: 모든 단어를 서브워드 조합으로 표현 가능\n",
    "        - **어휘 사전 크기 최적화**: 단어 단위보다 작고, 문자 단위보다 효율적\n",
    "        - **다국어 지원**: 언어에 구애받지 않는 범용적 토큰화\n",
    "        - **형태론적 의미 포착**: 접두사, 접미사 등의 의미를 학습 가능\n",
    "    - **주요 알고리즘**:\n",
    "        - **BPE (Byte Pair Encoding)**: 가장 빈번한 연속 바이트/문자 쌍을 반복적으로 병합\n",
    "        - **WordPiece**: BERT에서 사용, BPE와 유사하지만 likelihood 기반으로 병합\n",
    "        - **Unigram**: 확률 모델 기반으로 최적의 서브워드 분할 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1a834",
   "metadata": {},
   "source": [
    "# 한국어 형태소 분석기\n",
    "\n",
    "- kiwipiepy와 konlpy 는 대표적인 한국어 형태소 분석기이다.\n",
    "\n",
    "## kiwipiepy\n",
    "**kiwipiepy**는 C++로 구현된 한국어 형태소 분석기 Kiwi(Korean Intelligent Word Identifier)를 Python에서 사용할 수 있도록 한 라이브러리. \n",
    "\n",
    "- 빠른 속도  \n",
    "- 최신 품사 체계 지원  \n",
    "- 사용자 사전 확장 용이  \n",
    "- 최근 가장 널리 쓰이는 한국어 토크나이저 중 하나이다.\n",
    "- https://github.com/bab2min/kiwipiepy\n",
    "  \n",
    "### 설치 방법\n",
    "\n",
    "```bash\n",
    "pip install kiwipiepy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44226128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install ipykernel ipywidgets\n",
    "\n",
    "# 하다가 kiwipiepy 안되면 \n",
    "# uv pip uninstall kiwipiepy\n",
    "# uv pip install kiwipiepy==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07544102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kiwipiepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87abcc",
   "metadata": {},
   "source": [
    "### 주요 클래스 및 함수\n",
    "\n",
    "#### Kiwi 클래스\n",
    "- Kiwi의 핵심 클래스이며, 형태소 분석과 토큰화 기능을 모두 제공.\n",
    "- Kiwi 품사는 세종 말뭉치를 기반으로 한다.\n",
    "  - 품사 시작 글자\n",
    "  - 체언(명사, 대명사): `N`, 용언(동사, 형용사): `V`, 수식언(관형사, 부사): `M`,  관계언(조사):`J`, 어미: `E`, 기호: `S`\n",
    "    - https://github.com/bab2min/kiwipiepy?tab=readme-ov-file#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8\n",
    "- 메소드\n",
    "  - `tokenize(text)`: 형태소 분석 기반 토큰화 수행\n",
    "  - `analyze(text)`: tokenize보다 좀 더 상세한 분석을 진행한다. 여러 분석결과를 조회할 수있다.\n",
    "  - `add_user_word(word, pos, score)`: 사전에 직접 단어 등록\n",
    "  - `space(text)`: 띄어 쓰기 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd597bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from pprint import pprint  # 자료 구조 출력을 보기 좋게 print 해 줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4b5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d348d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token(form='나', tag='NP', start=0, len=1),\n",
      " Token(form='는', tag='JX', start=1, len=1),\n",
      " Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      " Token(form='를', tag='JKO', start=9, len=1),\n",
      " Token(form='공부', tag='NNG', start=11, len=2),\n",
      " Token(form='하', tag='XSV', start=13, len=1),\n",
      " Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      " Token(form='.', tag='SF', start=15, len=1),\n",
      " Token(form='내일', tag='NNG', start=19, len=2),\n",
      " Token(form='은', tag='JX', start=21, len=1),\n",
      " Token(form='뭐', tag='NP', start=23, len=1),\n",
      " Token(form='ᆯ', tag='JKO', start=23, len=1),\n",
      " Token(form='공부', tag='NNG', start=25, len=2),\n",
      " Token(form='하', tag='XSV', start=27, len=1),\n",
      " Token(form='ᆯ까', tag='EF', start=27, len=2),\n",
      " Token(form='?', tag='SF', start=29, len=1)]\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# 토큰화 - tokernize()\n",
    "########################\n",
    "text = \"나는 자연어 처리를 공부한다. \\n 내일은 뭘 공부할까?\"\n",
    "tokens = kiwi.tokenize(text)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2877f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰문자열: 나, 원형(lemma): 나, 품사(Tag): NP, 시작위치: 0, 글자수: 1, 토큰이 있는 행 번호: 0, 몇 번째 문장에 있는지: 0, 문장에서 몇 번째 어절인지: 0\n"
     ]
    }
   ],
   "source": [
    "# Token 객체에서 속성값들 조회\n",
    "for token in tokens:\n",
    "    r = f\"토큰문자열: {token.form}, 원형(lemma): {token.lemma}, 품사(Tag): {token.tag}, 시작위치: {token.start}, 글자수: {token.len}, 토큰이 있는 행 번호: {token.line_number}, 몇 번째 문장에 있는지: {token.sent_position}, 문장에서 몇 번째 어절인지: {token.word_position}\"\n",
    "    print(r)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15272ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x10c8707f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 문서를 토큰화 할 때는 list로 묶어서 전달.\n",
    "# 결과 -> Iterable -> 한번에 한 문서의 결과를 반환. \n",
    "text_list = [\"나는 자연어 처리를 공부한다. 자연어처리는 NLP라고 한다.\", \"내일은 뭘 공부할까?\"]\n",
    "tokens = kiwi.tokenize(text_list)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5baa2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token(form='나', tag='NP', start=0, len=1), Token(form='는', tag='JX', start=1, len=1), Token(form='자연어 처리', tag='NNP', start=3, len=6), Token(form='를', tag='JKO', start=9, len=1), Token(form='공부', tag='NNG', start=11, len=2), Token(form='하', tag='XSV', start=13, len=1), Token(form='ᆫ다', tag='EF', start=13, len=2), Token(form='.', tag='SF', start=15, len=1), Token(form='자연어 처리', tag='NNP', start=17, len=5), Token(form='는', tag='JX', start=22, len=1), Token(form='NLP', tag='SL', start=24, len=3), Token(form='이', tag='VCP', start=27, len=0), Token(form='라고', tag='EC', start=27, len=2), Token(form='하', tag='VV', start=30, len=1), Token(form='ᆫ다', tag='EF', start=30, len=2), Token(form='.', tag='SF', start=32, len=1)]\n",
      "[Token(form='내일', tag='NNG', start=0, len=2), Token(form='은', tag='JX', start=2, len=1), Token(form='뭐', tag='NP', start=4, len=1), Token(form='ᆯ', tag='JKO', start=4, len=1), Token(form='공부', tag='NNG', start=6, len=2), Token(form='하', tag='XSV', start=8, len=1), Token(form='ᆯ까', tag='EF', start=8, len=2), Token(form='?', tag='SF', start=10, len=1)]\n"
     ]
    }
   ],
   "source": [
    "for token_list in tokens:\n",
    "    print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e373d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1)],\n",
      "  -32.720970153808594),\n",
      " ([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어', tag='NNP', start=3, len=3),\n",
      "   Token(form='처리', tag='NNG', start=7, len=2),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1)],\n",
      "  -39.91743469238281),\n",
      " ([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EC', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1)],\n",
      "  -40.270137786865234)]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 상세한 토큰화 분석 - analyze()\n",
    "############################\n",
    "text = \"나는 자연어 처리를 공부한다.\"\n",
    "result = kiwi.analyze(text, top_n=3)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba4197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=17, len=5),\n",
      "   Token(form='는', tag='JX', start=22, len=1),\n",
      "   Token(form='NLP', tag='SL', start=24, len=3),\n",
      "   Token(form='이', tag='VCP', start=27, len=0),\n",
      "   Token(form='라고', tag='EC', start=27, len=2),\n",
      "   Token(form='하', tag='VV', start=30, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=30, len=2),\n",
      "   Token(form='.', tag='SF', start=32, len=1)],\n",
      "  -62.20681381225586),\n",
      " ([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어', tag='NNP', start=3, len=3),\n",
      "   Token(form='처리', tag='NNG', start=7, len=2),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1),\n",
      "   Token(form='자연어', tag='NNP', start=17, len=3),\n",
      "   Token(form='처리', tag='NNG', start=20, len=2),\n",
      "   Token(form='는', tag='JX', start=22, len=1),\n",
      "   Token(form='NLP', tag='SL', start=24, len=3),\n",
      "   Token(form='이', tag='VCP', start=27, len=0),\n",
      "   Token(form='라고', tag='EC', start=27, len=2),\n",
      "   Token(form='하', tag='VV', start=30, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=30, len=2),\n",
      "   Token(form='.', tag='SF', start=32, len=1)],\n",
      "  -75.48959350585938)]\n",
      "[([Token(form='내일', tag='NNG', start=0, len=2),\n",
      "   Token(form='은', tag='JX', start=2, len=1),\n",
      "   Token(form='뭐', tag='NP', start=4, len=1),\n",
      "   Token(form='ᆯ', tag='JKO', start=4, len=1),\n",
      "   Token(form='공부', tag='NNG', start=6, len=2),\n",
      "   Token(form='하', tag='XSV', start=8, len=1),\n",
      "   Token(form='ᆯ까', tag='EF', start=8, len=2),\n",
      "   Token(form='?', tag='SF', start=10, len=1)],\n",
      "  -33.69843292236328),\n",
      " ([Token(form='내일', tag='MAG', start=0, len=2),\n",
      "   Token(form='은', tag='JX', start=2, len=1),\n",
      "   Token(form='뭐', tag='NP', start=4, len=1),\n",
      "   Token(form='ᆯ', tag='JKO', start=4, len=1),\n",
      "   Token(form='공부', tag='NNG', start=6, len=2),\n",
      "   Token(form='하', tag='XSV', start=8, len=1),\n",
      "   Token(form='ᆯ까', tag='EF', start=8, len=2),\n",
      "   Token(form='?', tag='SF', start=10, len=1)],\n",
      "  -38.368492126464844)]\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"나는 자연어 처리를 공부한다. 자연어처리는 NLP라고 한다.\", \"내일은 뭘 공부할까?\"]\n",
    "result = kiwi. analyze(text_list, top_n=2)\n",
    "# result\n",
    "for r in result:\n",
    "    pprint(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42150e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어 처리는 재미있는 분야이다. 또 재미있는 것은 뭐가 있을까?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "# 띄어쓰기 교정 - space()\n",
    "######################\n",
    "text = \"자연어처리는재미있는분야이다.또재미있는것은뭐가있을까?\"\n",
    "result = kiwi.space(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c9d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 자연어 처리를 공부한다. 자연어 처리는 NLP라고 한다.\n",
      "내일은 뭘 공부할까?\n"
     ]
    }
   ],
   "source": [
    "test_list = [\"자연어처리는재미있는분야이다.또재미있는것은뭐가있을까?\", \"아버지가방에들어가신다\"]\n",
    "result = kiwi.space(text_list)\n",
    "result\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999fa253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(text='LG전자가주주환원을위해2,000억원을추가투입하기로했습니다', start=0, end=31, tokens=None, subs=[]),\n",
       " Sentence(text='지주사인 ㈜LG도 내년 상반기 자사주 전량 2,500억원어치를 소각하기로 하는 등 LG그룹 계열사들이 기업가치 제고 계획을 제시했습니다', start=31, end=106, tokens=None, subs=[]),\n",
       " Sentence(text='LG전자는 오늘(28일) 앞으로 2년간 2,000억원 규모 주주환원에 나서기로 하는 등의 내용을 담은 기업가치 제고 계획 이행현황을 공시했습니다.', start=106, end=187, tokens=None, subs=[]),\n",
       " Sentence(text='주주환원의 방법과 시기는 추후 이사회를 통해 결정하고 시장과 추가 소통할 계획입니다.', start=188, end=235, tokens=None, subs=[])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "# 문장분리 - split_into_sents()\n",
    "##############################\n",
    "txt = \"LG전자가주주환원을위해2,000억원을추가투입하기로했습니다지주사인 ㈜LG도 내년 상반기 자사주 전량 2,500억원어치를 소각하기로 하는 등 LG그룹 계열사들이 기업가치 제고 계획을 제시했습니다LG전자는 오늘(28일) 앞으로 2년간 2,000억원 규모 주주환원에 나서기로 하는 등의 내용을 담은 기업가치 제고 계획 이행현황을 공시했습니다. 주주환원의 방법과 시기는 추후 이사회를 통해 결정하고 시장과 추가 소통할 계획입니다.\"\n",
    "result = kiwi.split_into_sents(txt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e73fd5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG전자가 주주 환원을 위해 2,000 억 원을 추가 투입하기로 했습니다\n",
      "지주사인 ㈜LG도 내년 상반기 자사 주 전량 2,500 억 원 어치를 소각하기로 하는 등 LG 그룹 계열사들이 기업 가치 제고 계획을 제시했습니다\n",
      "LG전자는 오늘(28일) 앞으로 2년간 2,000 억 원 규모 주주 환원에 나서기로 하는 등의 내용을 담은 기업 가치 제고 계획 이행 현황을 공시했습니다.\n",
      "주주 환원의 방법과 시기는 추후 이사회를 통해 결정하고 시장과 추가 소통할 계획입니다.\n"
     ]
    }
   ],
   "source": [
    "# 위 분리된 문장에서 띄어쓰기 교정\n",
    "for sent in result:\n",
    "    print(kiwi.space(sent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357586ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(text=\"어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\", start=0, end=28, tokens=None, subs=[Sentence(text='집에 가고 싶다.', start=8, end=17, tokens=None, subs=None)])]\n"
     ]
    }
   ],
   "source": [
    "txt = \"어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\"\n",
    "result = kiwi.split_into_sents(txt)  # 결과:list[Sentence]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75414b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\n"
     ]
    }
   ],
   "source": [
    "print(result[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8969bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "집에 가고 싶다.\n"
     ]
    }
   ],
   "source": [
    "sent = result[0]\n",
    "if sent.subs != None:\n",
    "    print(sent.subs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d009b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='박', tag='NNP', start=0, len=1),\n",
       " Token(form='새로', tag='MAG', start=1, len=2),\n",
       " Token(form='이', tag='XSN', start=3, len=1),\n",
       " Token(form='가', tag='JKS', start=4, len=1),\n",
       " Token(form='오', tag='VV', start=6, len=1),\n",
       " Token(form='었', tag='EP', start=6, len=1),\n",
       " Token(form='다', tag='EF', start=7, len=1),\n",
       " Token(form='.', tag='SF', start=8, len=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 사전에 사용자 단어 추가 - add_user_word(단어, 품사, score)\n",
    "# - score: 토큰화할 때 그 단어의 우선순위를 조절하는 가중치 값. 클수록 더 선호하게 된다. \n",
    "#          0: 중립값. 고유명사들은 0을 지정.\n",
    "# - 딥러닝: 5, 딥: 10, 러닝: 10 -> 딥, 러닝으로 나누는 것을 더 선호하게 된다.\n",
    "##########################################################################\n",
    "text = \"박새로이가 왔다.\"\n",
    "kiwi.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='박새로이', tag='NP', start=0, len=4),\n",
       " Token(form='가', tag='JKS', start=4, len=1),\n",
       " Token(form='오', tag='VV', start=6, len=1),\n",
       " Token(form='었', tag='EP', start=6, len=1),\n",
       " Token(form='다', tag='EF', start=7, len=1),\n",
       " Token(form='.', tag='SF', start=8, len=1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi.add_user_word(\"박새로이\", \"NP\", 0)\n",
    "kiwi.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2c7dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# 불용어 (Stop words) 처리 - 토큰화했을 때 제거할 토큰 (단어)들\n",
    "#####################################################\n",
    "\n",
    "# kiwi에서 제공하는 불용어\n",
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "sw = Stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36a08470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ᆫ', 'ETM'),\n",
       " ('ᆫ', 'JX'),\n",
       " ('ᆫ다', 'EF'),\n",
       " ('ᆯ', 'ETM'),\n",
       " ('가', 'JKS'),\n",
       " ('같', 'VA'),\n",
       " ('것', 'NNB'),\n",
       " ('게', 'EC'),\n",
       " ('겠', 'EP'),\n",
       " ('고', 'EC'),\n",
       " ('고', 'JKQ'),\n",
       " ('과', 'JC'),\n",
       " ('과', 'JKB'),\n",
       " ('그', 'MM'),\n",
       " ('그', 'NP'),\n",
       " ('기', 'ETN'),\n",
       " ('까지', 'JX'),\n",
       " ('나', 'NP'),\n",
       " ('년', 'NNB'),\n",
       " ('는', 'ETM'),\n",
       " ('는', 'JX'),\n",
       " ('다', 'EC'),\n",
       " ('다', 'EF'),\n",
       " ('다고', 'EC'),\n",
       " ('다는', 'ETM'),\n",
       " ('대하', 'VV'),\n",
       " ('더', 'MAG'),\n",
       " ('던', 'ETM'),\n",
       " ('도', 'JX'),\n",
       " ('되', 'VV'),\n",
       " ('되', 'XSV'),\n",
       " ('들', 'XSN'),\n",
       " ('등', 'NNB'),\n",
       " ('따르', 'VV'),\n",
       " ('때', 'NNG'),\n",
       " ('때문', 'NNB'),\n",
       " ('라', 'EC'),\n",
       " ('라는', 'ETM'),\n",
       " ('로', 'JKB'),\n",
       " ('를', 'JKO'),\n",
       " ('만', 'JX'),\n",
       " ('만', 'NR'),\n",
       " ('말', 'NNG'),\n",
       " ('며', 'EC'),\n",
       " ('면', 'EC'),\n",
       " ('면서', 'EC'),\n",
       " ('명', 'NNB'),\n",
       " ('받', 'VV'),\n",
       " ('보', 'VV'),\n",
       " ('부터', 'JX'),\n",
       " ('사람', 'NNG'),\n",
       " ('성', 'XSN'),\n",
       " ('수', 'NNB'),\n",
       " ('아니', 'VCN'),\n",
       " ('않', 'VX'),\n",
       " ('어', 'EC'),\n",
       " ('어', 'EF'),\n",
       " ('어서', 'EC'),\n",
       " ('어야', 'EC'),\n",
       " ('없', 'VA'),\n",
       " ('었', 'EP'),\n",
       " ('에', 'JKB'),\n",
       " ('에게', 'JKB'),\n",
       " ('에서', 'JKB'),\n",
       " ('와', 'JC'),\n",
       " ('와', 'JKB'),\n",
       " ('우리', 'NP'),\n",
       " ('원', 'NNB'),\n",
       " ('월', 'NNB'),\n",
       " ('위하', 'VV'),\n",
       " ('으로', 'JKB'),\n",
       " ('은', 'ETM'),\n",
       " ('은', 'JX'),\n",
       " ('을', 'ETM'),\n",
       " ('을', 'JKO'),\n",
       " ('의', 'JKG'),\n",
       " ('이', 'JKC'),\n",
       " ('이', 'JKS'),\n",
       " ('이', 'MM'),\n",
       " ('이', 'NP'),\n",
       " ('이', 'VCP'),\n",
       " ('일', 'NNB'),\n",
       " ('일', 'NNG'),\n",
       " ('있', 'VV'),\n",
       " ('있', 'VX'),\n",
       " ('적', 'XSN'),\n",
       " ('제', 'XPN'),\n",
       " ('주', 'VX'),\n",
       " ('중', 'NNB'),\n",
       " ('지', 'EC'),\n",
       " ('지', 'VX'),\n",
       " ('지만', 'EC'),\n",
       " ('지역', 'NNG'),\n",
       " ('통하', 'VV'),\n",
       " ('하', 'VV'),\n",
       " ('하', 'VX'),\n",
       " ('하', 'XSA'),\n",
       " ('하', 'XSV'),\n",
       " ('한', 'MM'),\n",
       " ('화', 'XSN')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 조회\n",
    "sw.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aee9d",
   "metadata": {},
   "source": [
    "## KoNLPy(코엔엘파이)\n",
    "- KoNLPY는 한국어 자연어 처리(Natural Language Processing) 파이썬 라이브러리.  한국어 처리를 위한 tokenize, 형태소 분석, 어간추출, 품사부착(POS Tagging) 등의 기능 제공. \n",
    "- http://KoNLPy.org/ko/latest/\n",
    "- 기존의 개발된 다양한 형태소 분석기를 통합해서 동일한 interface로 호출 할 수 있게 해준다.\n",
    "\n",
    "### KoNLPy 설치\n",
    "- 설치 순서\n",
    "  1. Java 실행환경 설치\n",
    "  2. JPype1 설치\n",
    "  3. koNLPy 설치\n",
    "\n",
    "1. **Java 설치**\n",
    "  - https://www.oracle.com/java/technologies/downloads/\n",
    "    - OS에 맞는 설치 버전을 다운받아 설치한다.\n",
    "    - MAC: ARM일 경우: **ARM64 CPU** - ARM64 DMG Installer, **Intel CPU**: x64 DMG Installer\n",
    "  - 시스템 환경변수 설정\n",
    "      - `JAVA_HOME` : 설치 경로 지정\n",
    "      - `Path` : `설치경로\\bin` 경로 지정\n",
    "\n",
    "2. **JPype1 설치**\n",
    "   - 파이썬에서 자바 모듈을 호출하기 위한 연동 패키지\n",
    "   - 설치: `pip install JPype1`\n",
    "\n",
    "3. **KoNLPy 설치**\n",
    "- `pip install konlpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a21250e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkonlpy\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c7d17",
   "metadata": {},
   "source": [
    "### 형태소 분석기/사전\n",
    "- 형태소 사전을 내장하고 있으며 형태소 분석 함수들을 제공하는 모듈\n",
    "\n",
    "#### KoNLPy 제공 형태소 분석기\n",
    "- Open Korean Text\n",
    "    - 트위터에서 개발\n",
    "    - https://github.com/open-korean-text/open-korean-text\n",
    "- Hannanum(한나눔)\n",
    "    - KAIST Semantic Web Research Center 에서 개발\n",
    "    - http://semanticweb.kaist.ac.kr/hannanum/\n",
    "- Kkma(꼬꼬마)\n",
    "    - 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.\n",
    "    - http://kkma.snu.ac.kr/\n",
    "- Komoran(코모란)\n",
    "    - Shineware에서 개발.\n",
    "    - 오픈소스버전과 유료버전이 있음\n",
    "    - https://github.com/shin285/KOMORAN\n",
    "- Mecab(메카브) \n",
    "    - 일본어용 형태소 분석기를 한국에서 사용할 수 있도록 수정\n",
    "    - windows에서는 설치가 안됨\n",
    "    - https://bitbucket.org/eunjeon/mecab-ko\n",
    "\n",
    "\n",
    "### 형태소 분석기 공통 메소드\n",
    "- `morphs(string)` : 형태소 단위로 토큰화(tokenize)\n",
    "- `nouns(string)` : 명사만 추출하여 토큰화(tokenize)    \n",
    "- `pos(string)`: 품사 부착\n",
    "    - 형태소 분석기 마다 사용하는 품사태그가 다르다.\n",
    "        - https://konlpy-ko.readthedocs.io/ko/v0.5.2/morph/\n",
    "- `tagset`: 형태소 분석기가 사용하는 품사태그 설명하는 속성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a9426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a80e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd51d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d66f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26e26ee",
   "metadata": {},
   "source": [
    "# WordCloud\n",
    "\n",
    "WordCloud는 텍스트 데이터에서 단어의 등장 빈도를 시각적으로 표현한 그래픽이다.\n",
    "- 특징\n",
    "  - 자주 등장하는 단어일수록 글자 크기가 커진다.\n",
    "  - 텍스트 전체의 주제를 직관적으로 파악할 수 있다.\n",
    "  - 문서의 핵심키워드를 빠르게 파악할 수 있어 텍스트 분석에서 탐색적 데이터 분석(EDA) 단계에서 자주 활용된다.\n",
    "- 설치\n",
    "  - `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b8a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088404c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9932a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81bd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
