{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63189da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/11(목) 11:08\n",
    "# 강사님 거 보고 오류난 거 해결하기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers AutoClass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "- 기본 모델 Loading\n",
    "    1. **AutoModel**\n",
    "       - 주어진 모델 이름에 맞는 사전 학습된 모델 자동으로 로드.\n",
    "       - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델 로드.\n",
    "    2. **AutoTokenizer**\n",
    "       - 해당 모델에 적합한 토크나이저를 자동으로 로드.\n",
    "       - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드.\n",
    "    3. **AutoConfig**\n",
    "       - 모델의 설정(config)을 자동으로 로드. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델 생성할 수있다.\n",
    "       - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "- Task 처리 모델 Loading\n",
    "    - Pretrained backbone 모델에 각 task 별 estimator layer를 추가한 모델 생성해 제공.\n",
    "    - 주요 모델들\n",
    "        1. **AutoModelForSequenceClassification**\n",
    "           - 시퀀스(Text) 분류 작업을 위한 모델 자동으로 로드.\n",
    "           - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "        2. **AutoModelForQuestionAnswering**\n",
    "           - 질문-응답 작업을 위한 모델 자동으로 로드.\n",
    "           - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "        3. **AutoModelForTokenClassification**\n",
    "           - 토큰 분류 작업(예: 개체명 인식)을 위한 모델 자동으로 로드.\n",
    "           - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712789f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "# model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "print(type(model))\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(type(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b629b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.json - 모델 아키텍쳐 관련 메타정보파일\n",
    "# config\n",
    "model2 = AutoModel.from_config(config) # 학습이 안 된 모델\n",
    "type(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e268665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "b_model = BertModel.from_pretrained(model_id)\n",
    "b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# raw-text -> (Tokenizer) ==> token ids ==> (model) ==> 결과 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcde2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 1037, 2879, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "raw_text = \"I am a boy.\"\n",
    "\n",
    "# 토큰화\n",
    "token = tokenizer(\n",
    "    raw_text,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f711e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token (결과)의 key들 조회\n",
    "dict(token).keys()\n",
    "# 결과: input_ids': 입력 토큰 id값. \n",
    "# 'token_type_ids': 입력이 문장 쌍일 때 문장을 구분하는 id. 0: 첫 번째 문장, 1: 두 번째 문장. \n",
    "#                   각 토큰이 어느 문장에 속했는지 토큰 별로 지정됨. \n",
    "#                   문장 쌍: QA(질문-지문)\n",
    "# 'attention_mask': 실제 문장이 토큰과 Padding을 구분한다. 토큰 위치 별로 지정. 0: padding, 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d37d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 1037, 2879, 1012,  102]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abacc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token['input_ids']), type(token['token_type_ids']), type(token['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a995c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### \n",
    "# token 결과 -> model\n",
    "\n",
    "# keyword 가변 인자에 맞춰서 name = value 형태로 입력. input_ids=[.....],\n",
    "# model에 맞춤 타입으로 입력. (pytorch 모델: torch.tensor)\n",
    "context_vector = model(**token) # BertModel -> feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e82b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.last_hidden_state.shape\n",
    "# 모든 입력 토큰들의 hidden state들\n",
    "# [batch:1, seq_len: 7, hidden_size(embedding vector): 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c54182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 문장 (문서)에 대한 context vector - 문장(문서)의 특성값. \n",
    "## Bert는 last_hidden_state에서 첫 번째 토큰 값을 가공(특성 추출)해서 context vector로 사용.\n",
    "token['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d1323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 모델을 한글 텍스트로 학습 시킨 Pretrained model.\n",
    "    - BERT는 Transformer의 Encoder 부분을 이용해 구현된 언어모델\n",
    "    - https://arxiv.org/abs/1810.04805 \n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저, 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461ed8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"beomi/kcbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab6763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.bert.tokenization_bert_fast.BertTokenizerFast,\n",
       " transformers.models.bert.modeling_bert.BertModel)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79734a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  8616,  9909,  9025,  4196, 23905, 21452,  4020,    17,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 문서: padding - pad 추가, truncation - 잘라내기 (토큰 수 맞추는 작업) 신경 쓸 필요 없다.\n",
    "tokens = tokenizer(\n",
    "    \"나는 어제 친구와 밥을 먹었다.\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# tokenizer(토큰화 할 문장): 모델에 입력할 수 있는 형태로 토큰 결과를 반환.\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abbf472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 어제 친구와 밥을 먹었다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"나는 어제 친구와 밥을 먹었다.\")  # input_id만 출력.\n",
    "tokenizer.decode([2, 8616, 9909, 9025, 4196, 23905, 21452, 4020, 17, 3]) # token들 -> 문장 \n",
    "tokenizer.decode(\n",
    "    [2, 8616, 9909, 9025, 4196, 23905, 21452, 4020, 17, 3],\n",
    "    skip_special_tokens=True    # 스페셜 토큰은 제거.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbb695e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '어'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tokenizer.convert_ids_to_tokens(\u001b[32m8616\u001b[39m)  \u001b[38;5;66;03m# 토큰 ID -> 토큰 문자열\u001b[39;00m\n\u001b[32m      3\u001b[39m tokenizer.convert_ids_to_tokens([\u001b[32m2\u001b[39m, \u001b[32m8616\u001b[39m, \u001b[32m9909\u001b[39m]) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m어제\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      6\u001b[39m tokenizer.convert_ids_to_tokens([\u001b[33m\"\u001b[39m\u001b[33m어제\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m너는\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m나를\u001b[39m\u001b[33m\"\u001b[39m]) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SKN21_mjy/09_Huggingface_transformers/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:428\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m    426\u001b[39m ids_to_skip = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.all_special_ids) \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     index = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids_to_skip:\n\u001b[32m    430\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: '어'"
     ]
    }
   ],
   "source": [
    "# 개별 토큰 관련 조회\n",
    "tokenizer.convert_ids_to_tokens(8616)  # 토큰 ID -> 토큰 문자열\n",
    "tokenizer.convert_ids_to_tokens([2, 8616, 9909]) \n",
    "\n",
    "tokenizer.convert_ids_to_tokens(\"어제\") \n",
    "tokenizer.convert_ids_to_tokens([\"어제\", \"너는\", \"나를\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5fb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99032fb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf5087a1-04bb-4b36-912f-c1c5bdd77101",
   "metadata": {},
   "source": [
    "### 입력값 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78342de-1a9c-4b70-a997-c7bca15432ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"안녕\",\n",
    "    \"Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다.\",\n",
    "    \"2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8539917",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenizer(\n",
    "    sentences\n",
    "    # return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99565463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [[2, 19017, 3], [2, 41, 4223, 4403, 4403, 18940, 39, 4243, 4773, 4226, 4008, 14583, 25061, 11, 22502, 12, 321, 10459, 4071, 9810, 11, 47, 4450, 4579, 12, 16029, 7971, 13064, 8097, 867, 4228, 4196, 16505, 4027, 13248, 7966, 22502, 12296, 4104, 22192, 4020, 17, 3], [2, 26182, 4113, 20684, 4130, 2451, 22088, 20002, 15999, 4266, 4103, 17564, 4408, 4053, 4038, 4196, 11202, 20323, 4130, 47, 4450, 4579, 16505, 4027, 13248, 4027, 13248, 8008, 17, 3]]\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_tokens['input_ids']), sent_tokens['input_ids'])\n",
    "print(len(sent_tokens['attention_mask']))\n",
    "print(len(sent_tokens['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 최대 길이 (토큰 수)를 지정. \n",
    "# truncation - 길이를 맞추기 위해서 토큰의 일부를 잘라내는 것.\n",
    "##           - True(\"longest\"): 배치 중에서 가장 긴 문서에 맞춘다. max_length 설정이 된 경우에는 max_length에 맞춘다. \n",
    "##           - False(\"do_not_truncate\"): 기본값. truncation을 안 함.\n",
    "# padding - 길이를 맞추기 위해서 [PAD] 토큰으로 채우는 것.\n",
    "# 순서: truncation -> padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 context vector 추출\n",
    "#### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 context vector 이다.\n",
    "    - 이 값은 **문장을 입력받아 처리하는 task**(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ce7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787c1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad062b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
